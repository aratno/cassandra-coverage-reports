<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ReplicaPlans.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">JaCoCo Cassandara Coverage Report</a> &gt; <a href="index.source.html" class="el_package">org.apache.cassandra.locator</a> &gt; <span class="el_source">ReplicaPlans.java</span></div><h1>ReplicaPlans.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.locator;

import com.carrotsearch.hppc.ObjectIntHashMap;
import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.collect.ArrayListMultimap;
import com.google.common.collect.HashMultimap;
import com.google.common.collect.Iterables;
import com.google.common.collect.ListMultimap;
import com.google.common.collect.Lists;
import com.google.common.collect.Multimap;

import org.apache.cassandra.config.DatabaseDescriptor;
import org.apache.cassandra.db.ConsistencyLevel;
import org.apache.cassandra.db.DecoratedKey;
import org.apache.cassandra.db.Keyspace;
import org.apache.cassandra.db.PartitionPosition;
import org.apache.cassandra.dht.AbstractBounds;
import org.apache.cassandra.dht.Token;
import org.apache.cassandra.exceptions.UnavailableException;
import org.apache.cassandra.gms.FailureDetector;
import org.apache.cassandra.index.Index;
import org.apache.cassandra.index.IndexStatusManager;
import org.apache.cassandra.schema.SchemaConstants;
import org.apache.cassandra.service.StorageService;
import org.apache.cassandra.service.reads.AlwaysSpeculativeRetryPolicy;
import org.apache.cassandra.service.reads.SpeculativeRetryPolicy;

import org.apache.cassandra.utils.FBUtilities;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ThreadLocalRandom;
import java.util.function.Consumer;
import java.util.function.Function;
import java.util.function.Predicate;

import javax.annotation.Nullable;

import static com.google.common.collect.Iterables.any;
import static com.google.common.collect.Iterables.filter;
import static org.apache.cassandra.db.ConsistencyLevel.EACH_QUORUM;
import static org.apache.cassandra.db.ConsistencyLevel.eachQuorumForRead;
import static org.apache.cassandra.db.ConsistencyLevel.eachQuorumForWrite;
import static org.apache.cassandra.db.ConsistencyLevel.localQuorumFor;
import static org.apache.cassandra.db.ConsistencyLevel.localQuorumForOurDc;
import static org.apache.cassandra.locator.Replicas.addToCountPerDc;
import static org.apache.cassandra.locator.Replicas.countInOurDc;
import static org.apache.cassandra.locator.Replicas.countPerDc;

<span class="nc" id="L74">public class ReplicaPlans</span>
{
<span class="fc" id="L76">    private static final Logger logger = LoggerFactory.getLogger(ReplicaPlans.class);</span>

    public static boolean isSufficientLiveReplicasForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints&lt;?&gt; liveReplicas)
    {
<span class="pc bpc" id="L80" title="4 of 5 branches missed.">        switch (consistencyLevel)</span>
        {
            case ANY:
                // local hint is acceptable, and local node is always live
<span class="nc" id="L84">                return true;</span>
            case LOCAL_ONE:
<span class="nc" id="L86">                return countInOurDc(liveReplicas).hasAtleast(1, 1);</span>
            case LOCAL_QUORUM:
<span class="nc" id="L88">                return countInOurDc(liveReplicas).hasAtleast(localQuorumForOurDc(replicationStrategy), 1);</span>
            case EACH_QUORUM:
<span class="nc bnc" id="L90" title="All 2 branches missed.">                if (replicationStrategy instanceof NetworkTopologyStrategy)</span>
                {
<span class="nc" id="L92">                    int fullCount = 0;</span>
<span class="nc" id="L93">                    Collection&lt;String&gt; dcs = ((NetworkTopologyStrategy) replicationStrategy).getDatacenters();</span>
<span class="nc bnc" id="L94" title="All 2 branches missed.">                    for (ObjectObjectCursor&lt;String, Replicas.ReplicaCount&gt; entry : countPerDc(dcs, liveReplicas))</span>
                    {
<span class="nc" id="L96">                        Replicas.ReplicaCount count = entry.value;</span>
<span class="nc bnc" id="L97" title="All 2 branches missed.">                        if (!count.hasAtleast(localQuorumFor(replicationStrategy, entry.key), 0))</span>
<span class="nc" id="L98">                            return false;</span>
<span class="nc" id="L99">                        fullCount += count.fullReplicas();</span>
<span class="nc" id="L100">                    }</span>
<span class="nc bnc" id="L101" title="All 2 branches missed.">                    return fullCount &gt; 0;</span>
                }
                // Fallthough on purpose for SimpleStrategy
            default:
<span class="fc bfc" id="L105" title="All 2 branches covered.">                return liveReplicas.size() &gt;= consistencyLevel.blockFor(replicationStrategy)</span>
<span class="pc bpc" id="L106" title="1 of 2 branches missed.">                        &amp;&amp; Replicas.countFull(liveReplicas) &gt; 0;</span>
        }
    }

    static void assureSufficientLiveReplicasForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints&lt;?&gt; liveReplicas) throws UnavailableException
    {
<span class="fc" id="L112">        assureSufficientLiveReplicas(replicationStrategy, consistencyLevel, liveReplicas, consistencyLevel.blockFor(replicationStrategy), 1);</span>
<span class="fc" id="L113">    }</span>
    static void assureSufficientLiveReplicasForWrite(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints&lt;?&gt; allLive, Endpoints&lt;?&gt; pendingWithDown) throws UnavailableException
    {
<span class="fc" id="L116">        assureSufficientLiveReplicas(replicationStrategy, consistencyLevel, allLive, consistencyLevel.blockForWrite(replicationStrategy, pendingWithDown), 0);</span>
<span class="fc" id="L117">    }</span>
    static void assureSufficientLiveReplicas(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, Endpoints&lt;?&gt; allLive, int blockFor, int blockForFullReplicas) throws UnavailableException
    {
<span class="fc bfc" id="L120" title="All 5 branches covered.">        switch (consistencyLevel)</span>
        {
            case ANY:
                // local hint is acceptable, and local node is always live
<span class="fc" id="L124">                break;</span>
            case LOCAL_ONE:
            {
<span class="fc" id="L127">                Replicas.ReplicaCount localLive = countInOurDc(allLive);</span>
<span class="pc bpc" id="L128" title="1 of 2 branches missed.">                if (!localLive.hasAtleast(blockFor, blockForFullReplicas))</span>
<span class="nc" id="L129">                    throw UnavailableException.create(consistencyLevel, 1, blockForFullReplicas, localLive.allReplicas(), localLive.fullReplicas());</span>
                break;
            }
            case LOCAL_QUORUM:
            {
<span class="fc" id="L134">                Replicas.ReplicaCount localLive = countInOurDc(allLive);</span>
<span class="fc bfc" id="L135" title="All 2 branches covered.">                if (!localLive.hasAtleast(blockFor, blockForFullReplicas))</span>
                {
<span class="pc bpc" id="L137" title="1 of 2 branches missed.">                    if (logger.isTraceEnabled())</span>
                    {
<span class="nc" id="L139">                        logger.trace(String.format(&quot;Local replicas %s are insufficient to satisfy LOCAL_QUORUM requirement of %d live replicas and %d full replicas in '%s'&quot;,</span>
<span class="nc" id="L140">                                                   allLive.filter(InOurDc.replicas()), blockFor, blockForFullReplicas, DatabaseDescriptor.getLocalDataCenter()));</span>
                    }
<span class="fc" id="L142">                    throw UnavailableException.create(consistencyLevel, blockFor, blockForFullReplicas, localLive.allReplicas(), localLive.fullReplicas());</span>
                }
                break;
            }
            case EACH_QUORUM:
<span class="fc bfc" id="L147" title="All 2 branches covered.">                if (replicationStrategy instanceof NetworkTopologyStrategy)</span>
                {
<span class="fc" id="L149">                    int total = 0;</span>
<span class="fc" id="L150">                    int totalFull = 0;</span>
<span class="fc" id="L151">                    Collection&lt;String&gt; dcs = ((NetworkTopologyStrategy) replicationStrategy).getDatacenters();</span>
<span class="fc bfc" id="L152" title="All 2 branches covered.">                    for (ObjectObjectCursor&lt;String, Replicas.ReplicaCount&gt; entry : countPerDc(dcs, allLive))</span>
                    {
<span class="fc" id="L154">                        int dcBlockFor = localQuorumFor(replicationStrategy, entry.key);</span>
<span class="fc" id="L155">                        Replicas.ReplicaCount dcCount = entry.value;</span>
<span class="fc bfc" id="L156" title="All 2 branches covered.">                        if (!dcCount.hasAtleast(dcBlockFor, 0))</span>
<span class="fc" id="L157">                            throw UnavailableException.create(consistencyLevel, entry.key, dcBlockFor, dcCount.allReplicas(), 0, dcCount.fullReplicas());</span>
<span class="fc" id="L158">                        totalFull += dcCount.fullReplicas();</span>
<span class="fc" id="L159">                        total += dcCount.allReplicas();</span>
<span class="fc" id="L160">                    }</span>
<span class="pc bpc" id="L161" title="1 of 2 branches missed.">                    if (totalFull &lt; blockForFullReplicas)</span>
<span class="nc" id="L162">                        throw UnavailableException.create(consistencyLevel, blockFor, total, blockForFullReplicas, totalFull);</span>
                    break;
                }
                // Fallthough on purpose for SimpleStrategy
            default:
<span class="fc" id="L167">                int live = allLive.size();</span>
<span class="fc" id="L168">                int full = Replicas.countFull(allLive);</span>
<span class="pc bpc" id="L169" title="1 of 4 branches missed.">                if (live &lt; blockFor || full &lt; blockForFullReplicas)</span>
                {
<span class="pc bpc" id="L171" title="1 of 2 branches missed.">                    if (logger.isTraceEnabled())</span>
<span class="nc" id="L172">                        logger.trace(&quot;Live nodes {} do not satisfy ConsistencyLevel ({} required)&quot;, Iterables.toString(allLive), blockFor);</span>
<span class="fc" id="L173">                    throw UnavailableException.create(consistencyLevel, blockFor, blockForFullReplicas, live, full);</span>
                }
                break;
        }
<span class="fc" id="L177">    }</span>

    /**
     * Construct a ReplicaPlan for writing to exactly one node, with CL.ONE. This node is *assumed* to be alive.
     */
    public static ReplicaPlan.ForWrite forSingleReplicaWrite(Keyspace keyspace, Token token, Replica replica)
    {
<span class="fc" id="L184">        EndpointsForToken one = EndpointsForToken.of(token, replica);</span>
<span class="fc" id="L185">        EndpointsForToken empty = EndpointsForToken.empty(token);</span>
<span class="fc" id="L186">        return new ReplicaPlan.ForWrite(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, empty, one, one, one);</span>
    }

    /**
     * A forwarding counter write is always sent to a single owning coordinator for the range, by the original coordinator
     * (if it is not itself an owner)
     */
    public static ReplicaPlan.ForWrite forForwardingCounterWrite(Keyspace keyspace, Token token, Replica replica)
    {
<span class="fc" id="L195">        return forSingleReplicaWrite(keyspace, token, replica);</span>
    }

    public static ReplicaPlan.ForWrite forLocalBatchlogWrite()
    {
<span class="fc" id="L200">        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();</span>
<span class="fc" id="L201">        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);</span>
<span class="fc" id="L202">        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());</span>

<span class="fc" id="L204">        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(</span>
<span class="fc" id="L205">                systemKeypsace.getReplicationStrategy(),</span>
<span class="fc" id="L206">                EndpointsForToken.of(token, localSystemReplica),</span>
<span class="fc" id="L207">                EndpointsForToken.empty(token)</span>
        );
<span class="fc" id="L209">        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);</span>
    }

    /**
     * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.
     * Note that the liveAndDown collection and live are equal to the provided endpoints.
     *
     * @param isAny if batch consistency level is ANY, in which case a local node will be picked
     */
    public static ReplicaPlan.ForWrite forBatchlogWrite(boolean isAny) throws UnavailableException
    {
        // A single case we write not for range or token, but multiple mutations to many tokens
<span class="nc" id="L221">        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();</span>

<span class="nc" id="L223">        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();</span>
<span class="nc" id="L224">        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();</span>
<span class="nc" id="L225">        Multimap&lt;String, InetAddressAndPort&gt; localEndpoints = HashMultimap.create(topology.getDatacenterRacks()</span>
<span class="nc" id="L226">                                                                                          .get(snitch.getLocalDatacenter()));</span>
        // Replicas are picked manually:
        //  - replicas should be alive according to the failure detector
        //  - replicas should be in the local datacenter
        //  - choose min(2, number of qualifying candiates above)
        //  - allow the local node to be the only replica only if it's a single-node DC
<span class="nc" id="L232">        Collection&lt;InetAddressAndPort&gt; chosenEndpoints = filterBatchlogEndpoints(snitch.getLocalRack(), localEndpoints);</span>

<span class="nc bnc" id="L234" title="All 4 branches missed.">        if (chosenEndpoints.isEmpty() &amp;&amp; isAny)</span>
<span class="nc" id="L235">            chosenEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());</span>

<span class="nc" id="L237">        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);</span>
<span class="nc" id="L238">        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(</span>
<span class="nc" id="L239">                systemKeypsace.getReplicationStrategy(),</span>
<span class="nc" id="L240">                SystemReplicas.getSystemReplicas(chosenEndpoints).forToken(token),</span>
<span class="nc" id="L241">                EndpointsForToken.empty(token)</span>
        );
        // Batchlog is hosted by either one node or two nodes from different racks.
<span class="nc bnc" id="L244" title="All 2 branches missed.">        ConsistencyLevel consistencyLevel = liveAndDown.all().size() == 1 ? ConsistencyLevel.ONE : ConsistencyLevel.TWO;</span>
        // assume that we have already been given live endpoints, and skip applying the failure detector
<span class="nc" id="L246">        return forWrite(systemKeypsace, consistencyLevel, liveAndDown, liveAndDown, writeAll);</span>
    }

    private static Collection&lt;InetAddressAndPort&gt; filterBatchlogEndpoints(String localRack,
                                                                          Multimap&lt;String, InetAddressAndPort&gt; endpoints)
    {
<span class="nc" id="L252">        return filterBatchlogEndpoints(localRack,</span>
                                       endpoints,
                                       Collections::shuffle,
                                       FailureDetector.isEndpointAlive,
<span class="nc" id="L256">                                       ThreadLocalRandom.current()::nextInt);</span>
    }

    // Collect a list of candidates for batchlog hosting. If possible these will be two nodes from different racks.
    @VisibleForTesting
    public static Collection&lt;InetAddressAndPort&gt; filterBatchlogEndpoints(String localRack,
                                                                         Multimap&lt;String, InetAddressAndPort&gt; endpoints,
                                                                         Consumer&lt;List&lt;?&gt;&gt; shuffle,
                                                                         Predicate&lt;InetAddressAndPort&gt; isAlive,
                                                                         Function&lt;Integer, Integer&gt; indexPicker)
    {
        // special case for single-node data centers
<span class="nc bnc" id="L268" title="All 2 branches missed.">        if (endpoints.values().size() == 1)</span>
<span class="nc" id="L269">            return endpoints.values();</span>

        // strip out dead endpoints and localhost
<span class="nc" id="L272">        ListMultimap&lt;String, InetAddressAndPort&gt; validated = ArrayListMultimap.create();</span>
<span class="nc bnc" id="L273" title="All 2 branches missed.">        for (Map.Entry&lt;String, InetAddressAndPort&gt; entry : endpoints.entries())</span>
        {
<span class="nc" id="L275">            InetAddressAndPort addr = entry.getValue();</span>
<span class="nc bnc" id="L276" title="All 4 branches missed.">            if (!addr.equals(FBUtilities.getBroadcastAddressAndPort()) &amp;&amp; isAlive.test(addr))</span>
<span class="nc" id="L277">                validated.put(entry.getKey(), entry.getValue());</span>
<span class="nc" id="L278">        }</span>

<span class="nc bnc" id="L280" title="All 2 branches missed.">        if (validated.size() &lt;= 2)</span>
<span class="nc" id="L281">            return validated.values();</span>

<span class="nc bnc" id="L283" title="All 2 branches missed.">        if (validated.size() - validated.get(localRack).size() &gt;= 2)</span>
        {
            // we have enough endpoints in other racks
<span class="nc" id="L286">            validated.removeAll(localRack);</span>
        }

<span class="nc bnc" id="L289" title="All 2 branches missed.">        if (validated.keySet().size() == 1)</span>
        {
            /*
             * we have only 1 `other` rack to select replicas from (whether it be the local rack or a single non-local rack)
             * pick two random nodes from there; we are guaranteed to have at least two nodes in the single remaining rack
             * because of the preceding if block.
             */
<span class="nc" id="L296">            List&lt;InetAddressAndPort&gt; otherRack = Lists.newArrayList(validated.values());</span>
<span class="nc" id="L297">            shuffle.accept(otherRack);</span>
<span class="nc" id="L298">            return otherRack.subList(0, 2);</span>
        }

        // randomize which racks we pick from if more than 2 remaining
        Collection&lt;String&gt; racks;
<span class="nc bnc" id="L303" title="All 2 branches missed.">        if (validated.keySet().size() == 2)</span>
        {
<span class="nc" id="L305">            racks = validated.keySet();</span>
        }
        else
        {
<span class="nc" id="L309">            racks = Lists.newArrayList(validated.keySet());</span>
<span class="nc" id="L310">            shuffle.accept((List&lt;?&gt;) racks);</span>
        }

        // grab a random member of up to two racks
<span class="nc" id="L314">        List&lt;InetAddressAndPort&gt; result = new ArrayList&lt;&gt;(2);</span>
<span class="nc bnc" id="L315" title="All 2 branches missed.">        for (String rack : Iterables.limit(racks, 2))</span>
        {
<span class="nc" id="L317">            List&lt;InetAddressAndPort&gt; rackMembers = validated.get(rack);</span>
<span class="nc" id="L318">            result.add(rackMembers.get(indexPicker.apply(rackMembers.size())));</span>
<span class="nc" id="L319">        }</span>

<span class="nc" id="L321">        return result;</span>
    }

    public static ReplicaPlan.ForWrite forReadRepair(Token token, ReplicaPlan&lt;?, ?&gt; readPlan) throws UnavailableException
    {
<span class="fc" id="L326">        return forWrite(readPlan.keyspace(), readPlan.consistencyLevel(), token, writeReadRepair(readPlan));</span>
    }

    public static ReplicaPlan.ForWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, Token token, Selector selector) throws UnavailableException
    {
<span class="fc" id="L331">        return forWrite(keyspace, consistencyLevel, ReplicaLayout.forTokenWriteLiveAndDown(keyspace, token), selector);</span>
    }

    @VisibleForTesting
    public static ReplicaPlan.ForWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, EndpointsForToken natural, EndpointsForToken pending, Predicate&lt;Replica&gt; isAlive, Selector selector) throws UnavailableException
    {
<span class="nc" id="L337">        return forWrite(keyspace, consistencyLevel, ReplicaLayout.forTokenWrite(keyspace.getReplicationStrategy(), natural, pending), isAlive, selector);</span>
    }

    public static ReplicaPlan.ForWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, Selector selector) throws UnavailableException
    {
<span class="fc" id="L342">        return forWrite(keyspace, consistencyLevel, liveAndDown, FailureDetector.isReplicaAlive, selector);</span>
    }

    private static ReplicaPlan.ForWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, Predicate&lt;Replica&gt; isAlive, Selector selector) throws UnavailableException
    {
<span class="fc" id="L347">        ReplicaLayout.ForTokenWrite live = liveAndDown.filter(isAlive);</span>
<span class="fc" id="L348">        return forWrite(keyspace, consistencyLevel, liveAndDown, live, selector);</span>
    }

    public static ReplicaPlan.ForWrite forWrite(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaLayout.ForTokenWrite liveAndDown, ReplicaLayout.ForTokenWrite live, Selector selector) throws UnavailableException
    {
<span class="pc bpc" id="L353" title="1 of 2 branches missed.">        assert liveAndDown.replicationStrategy() == live.replicationStrategy()</span>
               : &quot;ReplicaLayout liveAndDown and live should be derived from the same replication strategy.&quot;;
<span class="fc" id="L355">        AbstractReplicationStrategy replicationStrategy = liveAndDown.replicationStrategy();</span>
<span class="fc" id="L356">        EndpointsForToken contacts = selector.select(consistencyLevel, liveAndDown, live);</span>
<span class="fc" id="L357">        assureSufficientLiveReplicasForWrite(replicationStrategy, consistencyLevel, live.all(), liveAndDown.pending());</span>
<span class="fc" id="L358">        return new ReplicaPlan.ForWrite(keyspace, replicationStrategy, consistencyLevel, liveAndDown.pending(), liveAndDown.all(), live.all(), contacts);</span>
    }

    public interface Selector
    {
        /**
         * Select the {@code Endpoints} from {@param liveAndDown} and {@param live} to contact according to the consistency level.
         */
        &lt;E extends Endpoints&lt;E&gt;, L extends ReplicaLayout.ForWrite&lt;E&gt;&gt;
        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live);
    }

    /**
     * Select all nodes, transient or otherwise, as targets for the operation.
     *
     * This is may no longer be useful once we finish implementing transient replication support, however
     * it can be of value to stipulate that a location writes to all nodes without regard to transient status.
     */
<span class="fc" id="L376">    public static final Selector writeAll = new Selector()</span>
<span class="fc" id="L377">    {</span>
        @Override
        public &lt;E extends Endpoints&lt;E&gt;, L extends ReplicaLayout.ForWrite&lt;E&gt;&gt;
        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
        {
<span class="fc" id="L382">            return liveAndDown.all();</span>
        }
    };

    /**
     * Select all full nodes, live or down, as write targets.  If there are insufficient nodes to complete the write,
     * but there are live transient nodes, select a sufficient number of these to reach our consistency level.
     *
     * Pending nodes are always contacted, whether or not they are full.  When a transient replica is undergoing
     * a pending move to a new node, if we write (transiently) to it, this write would not be replicated to the
     * pending transient node, and so when completing the move, the write could effectively have not reached the
     * promised consistency level.
     */
<span class="fc" id="L395">    public static final Selector writeNormal = new Selector()</span>
<span class="fc" id="L396">    {</span>
        @Override
        public &lt;E extends Endpoints&lt;E&gt;, L extends ReplicaLayout.ForWrite&lt;E&gt;&gt;
        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
        {
<span class="pc bpc" id="L401" title="1 of 2 branches missed.">            if (!any(liveAndDown.all(), Replica::isTransient))</span>
<span class="fc" id="L402">                return liveAndDown.all();</span>

<span class="nc" id="L404">            ReplicaCollection.Builder&lt;E&gt; contacts = liveAndDown.all().newBuilder(liveAndDown.all().size());</span>
<span class="nc" id="L405">            contacts.addAll(filter(liveAndDown.natural(), Replica::isFull));</span>
<span class="nc" id="L406">            contacts.addAll(liveAndDown.pending());</span>

            /**
             * Per CASSANDRA-14768, we ensure we write to at least a QUORUM of nodes in every DC,
             * regardless of how many responses we need to wait for and our requested consistencyLevel.
             * This is to minimally surprise users with transient replication; with normal writes, we
             * soft-ensure that we reach QUORUM in all DCs we are able to, by writing to every node;
             * even if we don't wait for ACK, we have in both cases sent sufficient messages.
              */
<span class="nc" id="L415">            ObjectIntHashMap&lt;String&gt; requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());</span>
<span class="nc" id="L416">            addToCountPerDc(requiredPerDc, live.natural().filter(Replica::isFull), -1);</span>
<span class="nc" id="L417">            addToCountPerDc(requiredPerDc, live.pending(), -1);</span>

<span class="nc" id="L419">            IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();</span>
<span class="nc bnc" id="L420" title="All 2 branches missed.">            for (Replica replica : filter(live.natural(), Replica::isTransient))</span>
            {
<span class="nc" id="L422">                String dc = snitch.getDatacenter(replica);</span>
<span class="nc bnc" id="L423" title="All 2 branches missed.">                if (requiredPerDc.addTo(dc, -1) &gt;= 0)</span>
<span class="nc" id="L424">                    contacts.add(replica);</span>
<span class="nc" id="L425">            }</span>
<span class="nc" id="L426">            return contacts.build();</span>
        }
    };

    /**
     * TODO: Transient Replication C-14404/C-14665
     * TODO: We employ this even when there is no monotonicity to guarantee,
     *          e.g. in case of CL.TWO, CL.ONE with speculation, etc.
     *
     * Construct a read-repair write plan to provide monotonicity guarantees on any data we return as part of a read.
     *
     * Since this is not a regular write, this is just to guarantee future reads will read this data, we select only
     * the minimal number of nodes to meet the consistency level, and prefer nodes we contacted on read to minimise
     * data transfer.
     */
    public static Selector writeReadRepair(ReplicaPlan&lt;?, ?&gt; readPlan)
    {
<span class="fc" id="L443">        return new Selector()</span>
<span class="fc" id="L444">        {</span>
            @Override
            public &lt;E extends Endpoints&lt;E&gt;, L extends ReplicaLayout.ForWrite&lt;E&gt;&gt;
            E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
            {
<span class="pc bpc" id="L449" title="1 of 2 branches missed.">                assert !any(liveAndDown.all(), Replica::isTransient);</span>

<span class="fc" id="L451">                ReplicaCollection.Builder&lt;E&gt; contacts = live.all().newBuilder(live.all().size());</span>
                // add all live nodes we might write to that we have already contacted on read
<span class="fc" id="L453">                contacts.addAll(filter(live.all(), r -&gt; readPlan.contacts().endpoints().contains(r.endpoint())));</span>

                // finally, add sufficient nodes to achieve our consistency level
<span class="fc bfc" id="L456" title="All 2 branches covered.">                if (consistencyLevel != EACH_QUORUM)</span>
                {
<span class="fc" id="L458">                    int add = consistencyLevel.blockForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending()) - contacts.size();</span>
<span class="pc bpc" id="L459" title="1 of 2 branches missed.">                    if (add &gt; 0)</span>
                    {
<span class="nc bnc" id="L461" title="All 4 branches missed.">                        for (Replica replica : filter(live.all(), r -&gt; !contacts.contains(r)))</span>
                        {
<span class="nc" id="L463">                            contacts.add(replica);</span>
<span class="nc bnc" id="L464" title="All 2 branches missed.">                            if (--add == 0)</span>
<span class="nc" id="L465">                                break;</span>
<span class="nc" id="L466">                        }</span>
                    }
<span class="fc" id="L468">                }</span>
                else
                {
<span class="fc" id="L471">                    ObjectIntHashMap&lt;String&gt; requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());</span>
<span class="fc" id="L472">                    addToCountPerDc(requiredPerDc, contacts.snapshot(), -1);</span>
<span class="fc" id="L473">                    IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();</span>
<span class="fc bfc" id="L474" title="All 4 branches covered.">                    for (Replica replica : filter(live.all(), r -&gt; !contacts.contains(r)))</span>
                    {
<span class="fc" id="L476">                        String dc = snitch.getDatacenter(replica);</span>
<span class="pc bpc" id="L477" title="1 of 2 branches missed.">                        if (requiredPerDc.addTo(dc, -1) &gt;= 0)</span>
<span class="nc" id="L478">                            contacts.add(replica);</span>
<span class="fc" id="L479">                    }</span>
                }
<span class="fc" id="L481">                return contacts.build();</span>
            }
        };
    }

    /**
     * Construct the plan for a paxos round - NOT the write or read consistency level for either the write or comparison,
     * but for the paxos linearisation agreement.
     *
     * This will select all live nodes as the candidates for the operation.  Only the required number of participants
     */
    public static ReplicaPlan.ForPaxosWrite forPaxos(Keyspace keyspace, DecoratedKey key, ConsistencyLevel consistencyForPaxos) throws UnavailableException
    {
<span class="fc" id="L494">        Token tk = key.getToken();</span>

<span class="fc" id="L496">        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWriteLiveAndDown(keyspace, tk);</span>

<span class="fc" id="L498">        Replicas.temporaryAssertFull(liveAndDown.all()); // TODO CASSANDRA-14547</span>

<span class="fc bfc" id="L500" title="All 2 branches covered.">        if (consistencyForPaxos == ConsistencyLevel.LOCAL_SERIAL)</span>
        {
            // TODO: we should cleanup our semantics here, as we're filtering ALL nodes to localDC which is unexpected for ReplicaPlan
            // Restrict natural and pending to node in the local DC only
<span class="fc" id="L504">            liveAndDown = liveAndDown.filter(InOurDc.replicas());</span>
        }

<span class="fc" id="L507">        ReplicaLayout.ForTokenWrite live = liveAndDown.filter(FailureDetector.isReplicaAlive);</span>

        // TODO: this should use assureSufficientReplicas
<span class="fc" id="L510">        int participants = liveAndDown.all().size();</span>
<span class="fc" id="L511">        int requiredParticipants = participants / 2 + 1; // See CASSANDRA-8346, CASSANDRA-833</span>

<span class="fc" id="L513">        EndpointsForToken contacts = live.all();</span>
<span class="fc bfc" id="L514" title="All 2 branches covered.">        if (contacts.size() &lt; requiredParticipants)</span>
<span class="fc" id="L515">            throw UnavailableException.create(consistencyForPaxos, requiredParticipants, contacts.size());</span>

        // We cannot allow CAS operations with 2 or more pending endpoints, see #8346.
        // Note that we fake an impossible number of required nodes in the unavailable exception
        // to nail home the point that it's an impossible operation no matter how many nodes are live.
<span class="pc bpc" id="L520" title="1 of 2 branches missed.">        if (liveAndDown.pending().size() &gt; 1)</span>
<span class="nc" id="L521">            throw new UnavailableException(String.format(&quot;Cannot perform LWT operation as there is more than one (%d) pending range movement&quot;, liveAndDown.all().size()),</span>
                    consistencyForPaxos,
                    participants + 1,
<span class="nc" id="L524">                    contacts.size());</span>

<span class="fc" id="L526">        return new ReplicaPlan.ForPaxosWrite(keyspace, consistencyForPaxos, liveAndDown.pending(), liveAndDown.all(), live.all(), contacts, requiredParticipants);</span>
    }

    private static &lt;E extends Endpoints&lt;E&gt;&gt; E candidatesForRead(Keyspace keyspace,
                                                                @Nullable Index.QueryPlan indexQueryPlan,
                                                                ConsistencyLevel consistencyLevel,
                                                                E liveNaturalReplicas)
    {
<span class="fc bfc" id="L534" title="All 2 branches covered.">        E replicas = consistencyLevel.isDatacenterLocal() ? liveNaturalReplicas.filter(InOurDc.replicas()) : liveNaturalReplicas;</span>

<span class="pc bpc" id="L536" title="1 of 2 branches missed.">        return indexQueryPlan != null ? IndexStatusManager.instance.filterForQuery(replicas, keyspace, indexQueryPlan, consistencyLevel) : replicas;</span>
    }

    private static &lt;E extends Endpoints&lt;E&gt;&gt; E contactForEachQuorumRead(NetworkTopologyStrategy replicationStrategy, E candidates)
    {
<span class="fc" id="L541">        ObjectIntHashMap&lt;String&gt; perDc = eachQuorumForRead(replicationStrategy);</span>

<span class="fc" id="L543">        final IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();</span>
<span class="fc" id="L544">        return candidates.filter(replica -&gt; {</span>
<span class="fc" id="L545">            String dc = snitch.getDatacenter(replica);</span>
<span class="fc bfc" id="L546" title="All 2 branches covered.">            return perDc.addTo(dc, -1) &gt;= 0;</span>
        });
    }

    private static &lt;E extends Endpoints&lt;E&gt;&gt; E contactForRead(AbstractReplicationStrategy replicationStrategy, ConsistencyLevel consistencyLevel, boolean alwaysSpeculate, E candidates)
    {
        /*
         * If we are doing an each quorum query, we have to make sure that the endpoints we select
         * provide a quorum for each data center. If we are not using a NetworkTopologyStrategy,
         * we should fall through and grab a quorum in the replication strategy.
         *
         * We do not speculate for EACH_QUORUM.
         *
         * TODO: this is still very inconistently managed between {LOCAL,EACH}_QUORUM and other consistency levels - should address this in a follow-up
         */
<span class="fc bfc" id="L561" title="All 4 branches covered.">        if (consistencyLevel == EACH_QUORUM &amp;&amp; replicationStrategy instanceof NetworkTopologyStrategy)</span>
<span class="fc" id="L562">            return contactForEachQuorumRead((NetworkTopologyStrategy) replicationStrategy, candidates);</span>

<span class="pc bpc" id="L564" title="1 of 2 branches missed.">        int count = consistencyLevel.blockFor(replicationStrategy) + (alwaysSpeculate ? 1 : 0);</span>
<span class="fc" id="L565">        return candidates.subList(0, Math.min(count, candidates.size()));</span>
    }


    /**
     * Construct a plan for reading from a single node - this permits no speculation or read-repair
     */
    public static ReplicaPlan.ForTokenRead forSingleReplicaRead(Keyspace keyspace, Token token, Replica replica)
    {
<span class="fc" id="L574">        EndpointsForToken one = EndpointsForToken.of(token, replica);</span>
<span class="fc" id="L575">        return new ReplicaPlan.ForTokenRead(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, one, one);</span>
    }

    /**
     * Construct a plan for reading from a single node - this permits no speculation or read-repair
     */
    public static ReplicaPlan.ForRangeRead forSingleReplicaRead(Keyspace keyspace, AbstractBounds&lt;PartitionPosition&gt; range, Replica replica, int vnodeCount)
    {
        // TODO: this is unsafe, as one.range() may be inconsistent with our supplied range; should refactor Range/AbstractBounds to single class
<span class="nc" id="L584">        EndpointsForRange one = EndpointsForRange.of(replica);</span>
<span class="nc" id="L585">        return new ReplicaPlan.ForRangeRead(keyspace, keyspace.getReplicationStrategy(), ConsistencyLevel.ONE, range, one, one, vnodeCount);</span>
    }

    /**
     * Construct a plan for reading the provided token at the provided consistency level.  This translates to a collection of
     *   - candidates who are: alive, replicate the token, and are sorted by their snitch scores
     *   - contacts who are: the first blockFor + (retry == ALWAYS ? 1 : 0) candidates
     *
     * The candidate collection can be used for speculation, although at present
     * it would break EACH_QUORUM to do so without further filtering
     */
    public static ReplicaPlan.ForTokenRead forRead(Keyspace keyspace,
                                                   Token token,
                                                   @Nullable Index.QueryPlan indexQueryPlan,
                                                   ConsistencyLevel consistencyLevel,
                                                   SpeculativeRetryPolicy retry)
    {
<span class="fc" id="L602">        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();</span>
<span class="fc" id="L603">        EndpointsForToken candidates = candidatesForRead(keyspace, indexQueryPlan, consistencyLevel, ReplicaLayout.forTokenReadLiveSorted(replicationStrategy, token).natural());</span>
<span class="fc" id="L604">        EndpointsForToken contacts = contactForRead(replicationStrategy, consistencyLevel, retry.equals(AlwaysSpeculativeRetryPolicy.INSTANCE), candidates);</span>

<span class="fc" id="L606">        assureSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, contacts);</span>
<span class="fc" id="L607">        return new ReplicaPlan.ForTokenRead(keyspace, replicationStrategy, consistencyLevel, candidates, contacts);</span>
    }

    /**
     * Construct a plan for reading the provided range at the provided consistency level.  This translates to a collection of
     *   - candidates who are: alive, replicate the range, and are sorted by their snitch scores
     *   - contacts who are: the first blockFor candidates
     *
     * There is no speculation for range read queries at present, so we never 'always speculate' here, and a failed response fails the query.
     */
    public static ReplicaPlan.ForRangeRead forRangeRead(Keyspace keyspace,
                                                        @Nullable Index.QueryPlan indexQueryPlan,
                                                        ConsistencyLevel consistencyLevel,
                                                        AbstractBounds&lt;PartitionPosition&gt; range,
                                                        int vnodeCount)
    {
<span class="fc" id="L623">        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();</span>
<span class="fc" id="L624">        EndpointsForRange candidates = candidatesForRead(keyspace, indexQueryPlan, consistencyLevel, ReplicaLayout.forRangeReadLiveSorted(replicationStrategy, range).natural());</span>
<span class="fc" id="L625">        EndpointsForRange contacts = contactForRead(replicationStrategy, consistencyLevel, false, candidates);</span>

<span class="fc" id="L627">        assureSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, contacts);</span>
<span class="fc" id="L628">        return new ReplicaPlan.ForRangeRead(keyspace, replicationStrategy, consistencyLevel, range, candidates, contacts, vnodeCount);</span>
    }

    /**
     * Take two range read plans for adjacent ranges, and check if it is OK (and worthwhile) to combine them into a single plan
     */
    public static ReplicaPlan.ForRangeRead maybeMerge(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaPlan.ForRangeRead left, ReplicaPlan.ForRangeRead right)
    {
        // TODO: should we be asserting that the ranges are adjacent?
<span class="fc" id="L637">        AbstractBounds&lt;PartitionPosition&gt; newRange = left.range().withNewRight(right.range().right);</span>
<span class="fc" id="L638">        EndpointsForRange mergedCandidates = left.readCandidates().keep(right.readCandidates().endpoints());</span>
<span class="fc" id="L639">        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();</span>

        // Check if there are enough shared endpoints for the merge to be possible.
<span class="fc bfc" id="L642" title="All 2 branches covered.">        if (!isSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, mergedCandidates))</span>
<span class="fc" id="L643">            return null;</span>

<span class="fc" id="L645">        EndpointsForRange contacts = contactForRead(replicationStrategy, consistencyLevel, false, mergedCandidates);</span>

        // Estimate whether merging will be a win or not
<span class="pc bpc" id="L648" title="1 of 2 branches missed.">        if (!DatabaseDescriptor.getEndpointSnitch().isWorthMergingForRangeQuery(contacts, left.contacts(), right.contacts()))</span>
<span class="nc" id="L649">            return null;</span>

        // If we get there, merge this range and the next one
<span class="fc" id="L652">        return new ReplicaPlan.ForRangeRead(keyspace, replicationStrategy, consistencyLevel, newRange, mergedCandidates, contacts, left.vnodeCount() + right.vnodeCount());</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>
<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>SortedTableScrubber.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">JaCoCo Cassandara Coverage Report</a> &gt; <a href="index.source.html" class="el_package">org.apache.cassandra.io.sstable.format</a> &gt; <span class="el_source">SortedTableScrubber.java</span></div><h1>SortedTableScrubber.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.io.sstable.format;

import java.io.IOError;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Set;
import java.util.SortedSet;
import java.util.TreeSet;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;
import javax.annotation.concurrent.NotThreadSafe;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableSet;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.cassandra.db.ClusteringComparator;
import org.apache.cassandra.db.ColumnFamilyStore;
import org.apache.cassandra.db.DecoratedKey;
import org.apache.cassandra.db.LivenessInfo;
import org.apache.cassandra.db.compaction.CompactionInfo;
import org.apache.cassandra.db.compaction.CompactionManager;
import org.apache.cassandra.db.compaction.OperationType;
import org.apache.cassandra.db.lifecycle.LifecycleTransaction;
import org.apache.cassandra.db.partitions.ImmutableBTreePartition;
import org.apache.cassandra.db.partitions.Partition;
import org.apache.cassandra.db.rows.AbstractCell;
import org.apache.cassandra.db.rows.Cell;
import org.apache.cassandra.db.rows.ColumnData;
import org.apache.cassandra.db.rows.ComplexColumnData;
import org.apache.cassandra.db.rows.Row;
import org.apache.cassandra.db.rows.Rows;
import org.apache.cassandra.db.rows.Unfiltered;
import org.apache.cassandra.db.rows.UnfilteredRowIterator;
import org.apache.cassandra.db.rows.UnfilteredRowIterators;
import org.apache.cassandra.db.rows.WrappingUnfilteredRowIterator;
import org.apache.cassandra.exceptions.ConfigurationException;
import org.apache.cassandra.io.sstable.Component;
import org.apache.cassandra.io.sstable.Descriptor;
import org.apache.cassandra.io.sstable.IScrubber;
import org.apache.cassandra.io.sstable.SSTableIdentityIterator;
import org.apache.cassandra.io.sstable.SSTableRewriter;
import org.apache.cassandra.io.sstable.format.SSTableFormat.Components;
import org.apache.cassandra.io.sstable.metadata.StatsMetadata;
import org.apache.cassandra.io.util.File;
import org.apache.cassandra.io.util.RandomAccessReader;
import org.apache.cassandra.service.ActiveRepairService;
import org.apache.cassandra.utils.AbstractIterator;
import org.apache.cassandra.utils.ByteBufferUtil;
import org.apache.cassandra.utils.FBUtilities;
import org.apache.cassandra.utils.OutputHandler;
import org.apache.cassandra.utils.TimeUUID;
import org.apache.cassandra.utils.concurrent.Refs;
import org.apache.cassandra.utils.memory.HeapCloner;

import static org.apache.cassandra.utils.TimeUUID.Generator.nextTimeUUID;

@NotThreadSafe
public abstract class SortedTableScrubber&lt;R extends SSTableReaderWithFilter&gt; implements IScrubber
{
<span class="fc" id="L85">    private final static Logger logger = LoggerFactory.getLogger(SortedTableScrubber.class);</span>

    protected final ColumnFamilyStore cfs;
    protected final LifecycleTransaction transaction;
    protected final File destination;
    protected final IScrubber.Options options;
    protected final R sstable;
    protected final OutputHandler outputHandler;
    protected final boolean isCommutative;
    protected final long expectedBloomFilterSize;
<span class="nc" id="L95">    protected final ReadWriteLock fileAccessLock = new ReentrantReadWriteLock();</span>
    protected final RandomAccessReader dataFile;
    protected final ScrubInfo scrubInfo;

<span class="nc" id="L99">    protected final NegativeLocalDeletionInfoMetrics negativeLocalDeletionInfoMetrics = new NegativeLocalDeletionInfoMetrics();</span>

<span class="fc" id="L101">    private static final Comparator&lt;Partition&gt; partitionComparator = Comparator.comparing(Partition::partitionKey);</span>
<span class="nc" id="L102">    protected final SortedSet&lt;Partition&gt; outOfOrder = new TreeSet&lt;&gt;(partitionComparator);</span>


    protected int goodPartitions;
    protected int badPartitions;
    protected int emptyPartitions;


    protected SortedTableScrubber(ColumnFamilyStore cfs,
                                  LifecycleTransaction transaction,
                                  OutputHandler outputHandler,
                                  Options options)
<span class="nc" id="L114">    {</span>
<span class="nc" id="L115">        this.sstable = (R) transaction.onlyOne();</span>
<span class="nc" id="L116">        Preconditions.checkNotNull(sstable.metadata());</span>
<span class="nc bnc" id="L117" title="All 2 branches missed.">        assert sstable.metadata().keyspace.equals(cfs.getKeyspaceName());</span>
<span class="nc bnc" id="L118" title="All 2 branches missed.">        if (!sstable.descriptor.cfname.equals(cfs.metadata().name))</span>
        {
<span class="nc" id="L120">            logger.warn(&quot;Descriptor points to a different table {} than metadata {}&quot;, sstable.descriptor.cfname, cfs.metadata().name);</span>
        }
        try
        {
<span class="nc" id="L124">            sstable.metadata().validateCompatibility(cfs.metadata());</span>
        }
<span class="nc" id="L126">        catch (ConfigurationException ex)</span>
        {
<span class="nc" id="L128">            logger.warn(&quot;Descriptor points to a different table {} than metadata {}&quot;, sstable.descriptor.cfname, cfs.metadata().name);</span>
<span class="nc" id="L129">        }</span>

<span class="nc" id="L131">        this.cfs = cfs;</span>
<span class="nc" id="L132">        this.transaction = transaction;</span>
<span class="nc" id="L133">        this.outputHandler = outputHandler;</span>
<span class="nc" id="L134">        this.options = options;</span>
<span class="nc" id="L135">        this.destination = cfs.getDirectories().getLocationForDisk(cfs.getDiskBoundaries().getCorrectDiskForSSTable(sstable));</span>
<span class="nc" id="L136">        this.isCommutative = cfs.metadata().isCounter();</span>

<span class="nc" id="L138">        List&lt;SSTableReader&gt; toScrub = Collections.singletonList(sstable);</span>

        long approximateKeyCount;
        try
        {
<span class="nc" id="L143">            approximateKeyCount = SSTableReader.getApproximateKeyCount(toScrub);</span>
        }
<span class="nc" id="L145">        catch (RuntimeException ex)</span>
        {
<span class="nc" id="L147">            approximateKeyCount = 0;</span>
<span class="nc" id="L148">        }</span>
<span class="nc" id="L149">        this.expectedBloomFilterSize = Math.max(cfs.metadata().params.minIndexInterval, approximateKeyCount);</span>

        // loop through each partition, deserializing to check for damage.
        // We'll also loop through the index at the same time, using the position from the index to recover if the
        // partition header (key or data size) is corrupt. (This means our position in the index file will be one
        // partition &quot;ahead&quot; of the data file.)
<span class="nc bnc" id="L155" title="All 2 branches missed.">        this.dataFile = transaction.isOffline()</span>
<span class="nc" id="L156">                        ? sstable.openDataReader()</span>
<span class="nc" id="L157">                        : sstable.openDataReader(CompactionManager.instance.getRateLimiter());</span>

<span class="nc" id="L159">        this.scrubInfo = new ScrubInfo(dataFile, sstable, fileAccessLock.readLock());</span>

<span class="nc bnc" id="L161" title="All 2 branches missed.">        if (options.reinsertOverflowedTTLRows)</span>
<span class="nc" id="L162">            outputHandler.output(&quot;Starting scrub with reinsert overflowed TTL option&quot;);</span>
<span class="nc" id="L163">    }</span>

    public static void deleteOrphanedComponents(Descriptor descriptor, Set&lt;Component&gt; components)
    {
<span class="fc" id="L167">        File dataFile = descriptor.fileFor(Components.DATA);</span>
<span class="pc bpc" id="L168" title="2 of 4 branches missed.">        if (components.contains(Components.DATA) &amp;&amp; dataFile.length() &gt; 0)</span>
            // everything appears to be in order... moving on.
<span class="fc" id="L170">            return;</span>

        // missing the DATA file! all components are orphaned
<span class="nc" id="L173">        logger.warn(&quot;Removing orphans for {}: {}&quot;, descriptor, components);</span>
<span class="nc bnc" id="L174" title="All 2 branches missed.">        for (Component component : components)</span>
        {
<span class="nc" id="L176">            File file = descriptor.fileFor(component);</span>
<span class="nc bnc" id="L177" title="All 2 branches missed.">            if (file.exists())</span>
<span class="nc" id="L178">                descriptor.fileFor(component).delete();</span>
<span class="nc" id="L179">        }</span>
<span class="nc" id="L180">    }</span>

    @Override
    public void scrub()
    {
<span class="nc" id="L185">        List&lt;SSTableReader&gt; finished = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L186">        outputHandler.output(&quot;Scrubbing %s (%s)&quot;, sstable, FBUtilities.prettyPrintMemory(dataFile.length()));</span>
<span class="nc" id="L187">        try (SSTableRewriter writer = SSTableRewriter.construct(cfs, transaction, false, sstable.maxDataAge);</span>
<span class="nc" id="L188">             Refs&lt;SSTableReader&gt; refs = Refs.ref(Collections.singleton(sstable)))</span>
        {
<span class="nc" id="L190">            StatsMetadata metadata = sstable.getSSTableMetadata();</span>
<span class="nc" id="L191">            writer.switchWriter(CompactionManager.createWriter(cfs, destination, expectedBloomFilterSize, metadata.repairedAt, metadata.pendingRepair, metadata.isTransient, sstable, transaction));</span>

<span class="nc" id="L193">            scrubInternal(writer);</span>

<span class="nc bnc" id="L195" title="All 2 branches missed.">            if (!outOfOrder.isEmpty())</span>
<span class="nc" id="L196">                finished.add(writeOutOfOrderPartitions(metadata));</span>

            // finish obsoletes the old sstable
<span class="nc" id="L199">            transaction.obsoleteOriginals();</span>
<span class="nc bnc" id="L200" title="All 2 branches missed.">            finished.addAll(writer.setRepairedAt(badPartitions &gt; 0 ? ActiveRepairService.UNREPAIRED_SSTABLE : sstable.getSSTableMetadata().repairedAt).finish());</span>
        }
<span class="nc" id="L202">        catch (IOException ex)</span>
        {
<span class="nc" id="L204">            throw new RuntimeException(ex);</span>
        }
        finally
        {
<span class="nc bnc" id="L208" title="All 2 branches missed.">            if (transaction.isOffline())</span>
<span class="nc" id="L209">                finished.forEach(sstable -&gt; sstable.selfRef().release());</span>
        }

<span class="nc" id="L212">        outputSummary(finished);</span>
<span class="nc" id="L213">    }</span>

    protected abstract void scrubInternal(SSTableRewriter writer) throws IOException;

    private void outputSummary(List&lt;SSTableReader&gt; finished)
    {
<span class="nc bnc" id="L219" title="All 2 branches missed.">        if (!finished.isEmpty())</span>
        {
<span class="nc" id="L221">            outputHandler.output(&quot;Scrub of %s complete: %d partitions in new sstable and %d empty (tombstoned) partitions dropped&quot;, sstable, goodPartitions, emptyPartitions);</span>
<span class="nc bnc" id="L222" title="All 2 branches missed.">            if (negativeLocalDeletionInfoMetrics.fixedRows &gt; 0)</span>
<span class="nc" id="L223">                outputHandler.output(&quot;Fixed %d rows with overflowed local deletion time.&quot;, negativeLocalDeletionInfoMetrics.fixedRows);</span>
<span class="nc bnc" id="L224" title="All 2 branches missed.">            if (badPartitions &gt; 0)</span>
<span class="nc" id="L225">                outputHandler.warn(&quot;Unable to recover %d partitions that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any&quot;, badPartitions);</span>
        }
        else
        {
<span class="nc bnc" id="L229" title="All 2 branches missed.">            if (badPartitions &gt; 0)</span>
<span class="nc" id="L230">                outputHandler.warn(&quot;No valid partitions found while scrubbing %s; it is marked for deletion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot&quot;, sstable);</span>
            else
<span class="nc" id="L232">                outputHandler.output(&quot;Scrub of %s complete; looks like all %d partitions were tombstoned&quot;, sstable, emptyPartitions);</span>
        }
<span class="nc" id="L234">    }</span>

    private SSTableReader writeOutOfOrderPartitions(StatsMetadata metadata)
    {
        // out of order partitions/rows, but no bad partition found - we can keep our repairedAt time
<span class="nc bnc" id="L239" title="All 2 branches missed.">        long repairedAt = badPartitions &gt; 0 ? ActiveRepairService.UNREPAIRED_SSTABLE : sstable.getSSTableMetadata().repairedAt;</span>
        SSTableReader newInOrderSstable;
<span class="nc" id="L241">        try (SSTableWriter inOrderWriter = CompactionManager.createWriter(cfs, destination, expectedBloomFilterSize, repairedAt, metadata.pendingRepair, metadata.isTransient, sstable, transaction))</span>
        {
<span class="nc bnc" id="L243" title="All 2 branches missed.">            for (Partition partition : outOfOrder)</span>
<span class="nc" id="L244">                inOrderWriter.append(partition.unfilteredIterator());</span>
<span class="nc" id="L245">            inOrderWriter.setRepairedAt(-1);</span>
<span class="nc" id="L246">            inOrderWriter.setMaxDataAge(sstable.maxDataAge);</span>
<span class="nc" id="L247">            newInOrderSstable = inOrderWriter.finish(true);</span>
        }
<span class="nc" id="L249">        transaction.update(newInOrderSstable, false);</span>
<span class="nc" id="L250">        outputHandler.warn(&quot;%d out of order partition (or partitions without of order rows) found while scrubbing %s; &quot; +</span>
<span class="nc" id="L251">                           &quot;Those have been written (in order) to a new sstable (%s)&quot;, outOfOrder.size(), sstable, newInOrderSstable);</span>
<span class="nc" id="L252">        return newInOrderSstable;</span>
    }

    protected abstract UnfilteredRowIterator withValidation(UnfilteredRowIterator iter, String filename);

    @Override
    @VisibleForTesting
    public ScrubResult scrubWithResult()
    {
<span class="nc" id="L261">        scrub();</span>
<span class="nc" id="L262">        return new ScrubResult(goodPartitions, badPartitions, emptyPartitions);</span>
    }

    @Override
    public CompactionInfo.Holder getScrubInfo()
    {
<span class="nc" id="L268">        return scrubInfo;</span>
    }

    protected String keyString(DecoratedKey key)
    {
<span class="nc bnc" id="L273" title="All 2 branches missed.">        if (key == null)</span>
<span class="nc" id="L274">            return &quot;(unknown)&quot;;</span>

        try
        {
<span class="nc" id="L278">            return cfs.metadata().partitionKeyType.getString(key.getKey());</span>
        }
<span class="nc" id="L280">        catch (Exception e)</span>
        {
<span class="nc" id="L282">            return String.format(&quot;(corrupted; hex value: %s)&quot;, ByteBufferUtil.bytesToHex(key.getKey()));</span>
        }
    }

    protected boolean tryAppend(DecoratedKey prevKey, DecoratedKey key, SSTableRewriter writer)
    {
        // OrderCheckerIterator will check, at iteration time, that the rows are in the proper order. If it detects
        // that one row is out of order, it will stop returning them. The remaining rows will be sorted and added
        // to the outOfOrder set that will be later written to a new SSTable.
<span class="nc" id="L291">        try (OrderCheckerIterator sstableIterator = new OrderCheckerIterator(getIterator(key), cfs.metadata().comparator);</span>
<span class="nc" id="L292">             UnfilteredRowIterator iterator = withValidation(sstableIterator, dataFile.getPath()))</span>
        {
<span class="nc bnc" id="L294" title="All 4 branches missed.">            if (prevKey != null &amp;&amp; prevKey.compareTo(key) &gt; 0)</span>
            {
<span class="nc" id="L296">                saveOutOfOrderPartition(prevKey, key, iterator);</span>
<span class="nc" id="L297">                return false;</span>
            }

<span class="nc bnc" id="L300" title="All 2 branches missed.">            if (writer.tryAppend(iterator) == null)</span>
<span class="nc" id="L301">                emptyPartitions++;</span>
            else
<span class="nc" id="L303">                goodPartitions++;</span>

<span class="nc bnc" id="L305" title="All 2 branches missed.">            if (sstableIterator.hasRowsOutOfOrder())</span>
            {
<span class="nc" id="L307">                outputHandler.warn(&quot;Out of order rows found in partition: %s&quot;, keyString(key));</span>
<span class="nc" id="L308">                outOfOrder.add(sstableIterator.getRowsOutOfOrder());</span>
            }
<span class="nc bnc" id="L310" title="All 2 branches missed.">        }</span>

<span class="nc" id="L312">        return true;</span>
    }

    /**
     * Only wrap with {@link FixNegativeLocalDeletionTimeIterator} if {@link IScrubber.Options#reinsertOverflowedTTLRows} option
     * is specified
     */
    private UnfilteredRowIterator getIterator(DecoratedKey key)
    {
<span class="nc" id="L321">        RowMergingSSTableIterator rowMergingIterator = new RowMergingSSTableIterator(SSTableIdentityIterator.create(sstable,</span>
                                                                                                                    dataFile,
                                                                                                                    key),
                                                                                     outputHandler,
                                                                                     sstable.descriptor.version,
                                                                                     options.reinsertOverflowedTTLRows);
<span class="nc bnc" id="L327" title="All 2 branches missed.">        if (options.reinsertOverflowedTTLRows)</span>
<span class="nc" id="L328">            return new FixNegativeLocalDeletionTimeIterator(rowMergingIterator, outputHandler, negativeLocalDeletionInfoMetrics);</span>
        else
<span class="nc" id="L330">            return rowMergingIterator;</span>
    }

    private void saveOutOfOrderPartition(DecoratedKey prevKey, DecoratedKey key, UnfilteredRowIterator iterator)
    {
        // TODO bitch if the row is too large?  if it is there's not much we can do ...
<span class="nc" id="L336">        outputHandler.warn(&quot;Out of order partition detected (%s found after %s)&quot;, keyString(key), keyString(prevKey));</span>
<span class="nc" id="L337">        outOfOrder.add(ImmutableBTreePartition.create(iterator));</span>
<span class="nc" id="L338">    }</span>

    protected static void throwIfFatal(Throwable th)
    {
<span class="nc bnc" id="L342" title="All 6 branches missed.">        if (th instanceof Error &amp;&amp; !(th instanceof AssertionError || th instanceof IOError))</span>
<span class="nc" id="L343">            throw (Error) th;</span>
<span class="nc" id="L344">    }</span>

    protected void throwIfCannotContinue(DecoratedKey key, Throwable th)
    {
<span class="nc bnc" id="L348" title="All 4 branches missed.">        if (isCommutative &amp;&amp; !options.skipCorrupted)</span>
        {
<span class="nc" id="L350">            outputHandler.warn(&quot;An error occurred while scrubbing the partition with key '%s'.  Skipping corrupt &quot; +</span>
                               &quot;data in counter tables will result in undercounts for the affected &quot; +
                               &quot;counters (see CASSANDRA-2759 for more details), so by default the scrub will &quot; +
                               &quot;stop at this point.  If you would like to skip the row anyway and continue &quot; +
                               &quot;scrubbing, re-run the scrub with the --skip-corrupted option.&quot;,
<span class="nc" id="L355">                               keyString(key));</span>
<span class="nc" id="L356">            throw new IOError(th);</span>
        }
<span class="nc" id="L358">    }</span>


    public static class ScrubInfo extends CompactionInfo.Holder
    {
        private final RandomAccessReader dataFile;
        private final SSTableReader sstable;
        private final TimeUUID scrubCompactionId;
        private final Lock fileReadLock;

        public ScrubInfo(RandomAccessReader dataFile, SSTableReader sstable, Lock fileReadLock)
<span class="nc" id="L369">        {</span>
<span class="nc" id="L370">            this.dataFile = dataFile;</span>
<span class="nc" id="L371">            this.sstable = sstable;</span>
<span class="nc" id="L372">            this.fileReadLock = fileReadLock;</span>
<span class="nc" id="L373">            scrubCompactionId = nextTimeUUID();</span>
<span class="nc" id="L374">        }</span>

        public CompactionInfo getCompactionInfo()
        {
<span class="nc" id="L378">            fileReadLock.lock();</span>
            try
            {
<span class="nc" id="L381">                return new CompactionInfo(sstable.metadata(),</span>
                                          OperationType.SCRUB,
<span class="nc" id="L383">                                          dataFile.getFilePointer(),</span>
<span class="nc" id="L384">                                          dataFile.length(),</span>
                                          scrubCompactionId,
<span class="nc" id="L386">                                          ImmutableSet.of(sstable),</span>
<span class="nc" id="L387">                                          File.getPath(sstable.getFilename()).getParent().toString());</span>
            }
<span class="nc" id="L389">            catch (Exception e)</span>
            {
<span class="nc" id="L391">                throw new RuntimeException(e);</span>
            }
            finally
            {
<span class="nc" id="L395">                fileReadLock.unlock();</span>
            }
        }

        public boolean isGlobal()
        {
<span class="nc" id="L401">            return false;</span>
        }
    }

    /**
     * In some case like CASSANDRA-12127 the cells might have been stored in the wrong order. This decorator check the
     * cells order and collect the out-of-order cells to correct the problem.
     */
    private static final class OrderCheckerIterator extends AbstractIterator&lt;Unfiltered&gt; implements WrappingUnfilteredRowIterator
    {
        private final UnfilteredRowIterator iterator;
        private final ClusteringComparator comparator;

        private Unfiltered previous;

        /**
         * The partition containing the rows which are out of order.
         */
        private Partition rowsOutOfOrder;

        public OrderCheckerIterator(UnfilteredRowIterator iterator, ClusteringComparator comparator)
        {
            this.iterator = iterator;
            this.comparator = comparator;
        }

        @Override
        public UnfilteredRowIterator wrapped()
        {
            return iterator;
        }

        public boolean hasRowsOutOfOrder()
        {
            return rowsOutOfOrder != null;
        }

        public Partition getRowsOutOfOrder()
        {
            return rowsOutOfOrder;
        }

        @Override
        protected Unfiltered computeNext()
        {
            if (!iterator.hasNext())
                return endOfData();

            Unfiltered next = iterator.next();

            // If we detect that some rows are out of order we will store and sort the remaining ones to insert them
            // in a separate SSTable.
            if (previous != null &amp;&amp; comparator.compare(next, previous) &lt; 0)
            {
                rowsOutOfOrder = ImmutableBTreePartition.create(UnfilteredRowIterators.concat(next, iterator), false);
                return endOfData();
            }
            previous = next;
            return next;
        }
    }

    /**
     * During 2.x migration, under some circumstances rows might have gotten duplicated.
     * Merging iterator merges rows with same clustering.
     * &lt;p&gt;
     * For more details, refer to CASSANDRA-12144.
     */
    private static class RowMergingSSTableIterator implements WrappingUnfilteredRowIterator
    {
        Unfiltered nextToOffer = null;
        private final OutputHandler output;
        private final UnfilteredRowIterator wrapped;
        private final Version sstableVersion;
        private final boolean reinsertOverflowedTTLRows;

        RowMergingSSTableIterator(UnfilteredRowIterator source, OutputHandler output, Version sstableVersion, boolean reinsertOverflowedTTLRows)
        {
            this.wrapped = source;
            this.output = output;
            this.sstableVersion = sstableVersion;
            this.reinsertOverflowedTTLRows = reinsertOverflowedTTLRows;
        }

        @Override
        public UnfilteredRowIterator wrapped()
        {
            return wrapped;
        }

        @Override
        public boolean hasNext()
        {
            return nextToOffer != null || wrapped.hasNext();
        }

        @Override
        public Unfiltered next()
        {
            Unfiltered next = nextToOffer != null ? nextToOffer : wrapped.next();

            if (next.isRow())
            {
                boolean logged = false;
                while (wrapped.hasNext())
                {
                    Unfiltered peek = wrapped.next();
                    if (!peek.isRow() || !next.clustering().equals(peek.clustering()))
                    {
                        nextToOffer = peek; // Offer peek in next call
                        return computeFinalRow((Row) next);
                    }

                    // Duplicate row, merge it.
                    next = Rows.merge((Row) next, (Row) peek);

                    if (!logged)
                    {
                        String partitionKey = metadata().partitionKeyType.getString(partitionKey().getKey());
                        output.warn(&quot;Duplicate row detected in %s.%s: %s %s&quot;, metadata().keyspace, metadata().name, partitionKey, next.clustering().toString(metadata()));
                        logged = true;
                    }
                }
            }

            nextToOffer = null;
            return computeFinalRow((Row) next);
         }

         private Row computeFinalRow(Row next)
         {
             // If the row has overflowed let rows skip them unless we need to keep them for the overflow policy
             if (hasOverflowedLocalExpirationTimeRow(next) &amp;&amp; !reinsertOverflowedTTLRows)
                 return null;
             else if (reinsertOverflowedTTLRows)
                 return rebuildTimestamptsForOverflowedRows(next);
             else
                 return next;
         }

         /*
          * When building ldt on deser it won't overflow now being a long as it used to. 
          * This causes row resurrection for old sstable formats!
          * To prevent it we preserve the overflow to be backwards compatible and to feed into the overflow policy
          */
         private Row rebuildTimestamptsForOverflowedRows(Row row)
         {
             if (sstableVersion.hasUIntDeletionTime())
                 return row;

             LivenessInfo livenessInfo = row.primaryKeyLivenessInfo();
             if (livenessInfo.isExpiring() &amp;&amp; livenessInfo.localExpirationTime() &gt;= 0)
             {
                 livenessInfo = livenessInfo.withUpdatedTimestampAndLocalDeletionTime(livenessInfo.timestamp(), livenessInfo.localExpirationTime(), false);
             }

             return row.transformAndFilter(livenessInfo, row.deletion(), cd -&gt; {
                 if (cd.column().isSimple())
                 {
                     Cell&lt;?&gt; cell = (Cell&lt;?&gt;)cd;
                     return cell.isExpiring() &amp;&amp; cell.localDeletionTime() &gt;= 0
                            ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp(), cell.localDeletionTime())
                            : cell;
                 }
                 else
                 {
                     ComplexColumnData complexData = (ComplexColumnData)cd;
                     return complexData.transformAndFilter(cell -&gt; cell.isExpiring() &amp;&amp; cell.localDeletionTime() &gt;= 0
                                                                   ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp(), cell.localDeletionTime())
                                                                   : cell);
                 }
             }).clone(HeapCloner.instance);
         }

         private boolean hasOverflowedLocalExpirationTimeRow(Row next)
         {
             if (sstableVersion.hasUIntDeletionTime())
                 return false;

             if (next.primaryKeyLivenessInfo().isExpiring() &amp;&amp; next.primaryKeyLivenessInfo().localExpirationTime() &gt;= 0)
             {
                 return true;
             }

             for (ColumnData cd : next)
             {
                 if (cd.column().isSimple())
                 {
                     Cell&lt;?&gt; cell = (Cell&lt;?&gt;)cd;
                     if (cell.isExpiring() &amp;&amp; cell.localDeletionTime() &gt;= 0)
                         return true;
                 }
                 else
                 {
                     ComplexColumnData complexData = (ComplexColumnData)cd;
                     for (Cell&lt;?&gt; cell : complexData)
                     {
                         if (cell.isExpiring() &amp;&amp; cell.localDeletionTime() &gt;= 0)
                             return true;
                     }
                 }
             }

             return false;
         }
     }

    /**
     * This iterator converts negative {@link AbstractCell#localDeletionTime()} into {@link AbstractCell#MAX_DELETION_TIME}
     * &lt;p&gt;
     * This is to recover entries with overflowed localExpirationTime due to CASSANDRA-14092
     */
    private static final class FixNegativeLocalDeletionTimeIterator extends AbstractIterator&lt;Unfiltered&gt; implements WrappingUnfilteredRowIterator
    {
        /**
         * The decorated iterator.
         */
        private final UnfilteredRowIterator iterator;

        private final OutputHandler outputHandler;
        private final NegativeLocalDeletionInfoMetrics negativeLocalExpirationTimeMetrics;

        public FixNegativeLocalDeletionTimeIterator(UnfilteredRowIterator iterator, OutputHandler outputHandler,
                                                    NegativeLocalDeletionInfoMetrics negativeLocalDeletionInfoMetrics)
        {
            this.iterator = iterator;
            this.outputHandler = outputHandler;
            this.negativeLocalExpirationTimeMetrics = negativeLocalDeletionInfoMetrics;
        }

        @Override
        public UnfilteredRowIterator wrapped()
        {
            return iterator;
        }

        @Override
        protected Unfiltered computeNext()
        {
            if (!iterator.hasNext())
                return endOfData();

            Unfiltered next = iterator.next();
            if (!next.isRow())
                return next;

            if (hasNegativeLocalExpirationTime((Row) next))
            {
                outputHandler.debug(&quot;Found row with negative local expiration time: %s&quot;, next.toString(metadata(), false));
                negativeLocalExpirationTimeMetrics.fixedRows++;
                return fixNegativeLocalExpirationTime((Row) next);
            }

            return next;
        }

        private boolean hasNegativeLocalExpirationTime(Row next)
        {
            Row row = next;
            if (row.primaryKeyLivenessInfo().isExpiring() &amp;&amp; row.primaryKeyLivenessInfo().localExpirationTime() == Cell.INVALID_DELETION_TIME)
            {
                return true;
            }

            for (ColumnData cd : row)
            {
                if (cd.column().isSimple())
                {
                    Cell&lt;?&gt; cell = (Cell&lt;?&gt;) cd;
                    if (cell.isExpiring() &amp;&amp; cell.localDeletionTime() == Cell.INVALID_DELETION_TIME)
                        return true;
                }
                else
                {
                    ComplexColumnData complexData = (ComplexColumnData) cd;
                    for (Cell&lt;?&gt; cell : complexData)
                    {
                        if (cell.isExpiring() &amp;&amp; cell.localDeletionTime() == Cell.INVALID_DELETION_TIME)
                            return true;
                    }
                }
            }

            return false;
        }

        private Unfiltered fixNegativeLocalExpirationTime(Row row)
        {
            LivenessInfo livenessInfo = row.primaryKeyLivenessInfo();
            if (livenessInfo.isExpiring() &amp;&amp; livenessInfo.localExpirationTime() == Cell.INVALID_DELETION_TIME)
                livenessInfo = livenessInfo.withUpdatedTimestampAndLocalDeletionTime(livenessInfo.timestamp() + 1, AbstractCell.MAX_DELETION_TIME_2038_LEGACY_CAP);

            return row.transformAndFilter(livenessInfo, row.deletion(), cd -&gt; {
                if (cd.column().isSimple())
                {
                    Cell cell = (Cell) cd;
                    return cell.isExpiring() &amp;&amp; cell.localDeletionTime() == Cell.INVALID_DELETION_TIME
                           ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME_2038_LEGACY_CAP)
                           : cell;
                }
                else
                {
                    ComplexColumnData complexData = (ComplexColumnData) cd;
                    return complexData.transformAndFilter(cell -&gt; cell.isExpiring() &amp;&amp; cell.localDeletionTime() == Cell.INVALID_DELETION_TIME
                                                                  ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME_2038_LEGACY_CAP)
                                                                  : cell);
                }
            }).clone(HeapCloner.instance);
        }
    }

    private static class NegativeLocalDeletionInfoMetrics
    {
        public volatile int fixedRows = 0;
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>
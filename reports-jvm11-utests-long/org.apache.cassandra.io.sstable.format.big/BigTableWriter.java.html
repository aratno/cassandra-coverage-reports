<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>BigTableWriter.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">JaCoCo Cassandara Coverage Report</a> &gt; <a href="index.source.html" class="el_package">org.apache.cassandra.io.sstable.format.big</a> &gt; <span class="el_source">BigTableWriter.java</span></div><h1>BigTableWriter.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.cassandra.io.sstable.format.big;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Collection;
import java.util.HashMap;
import java.util.Map;
import java.util.function.Consumer;

import com.google.common.collect.ImmutableSet;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.cassandra.config.DatabaseDescriptor;
import org.apache.cassandra.db.DecoratedKey;
import org.apache.cassandra.db.DeletionTime;
import org.apache.cassandra.db.compaction.OperationType;
import org.apache.cassandra.db.lifecycle.ILifecycleTransaction;
import org.apache.cassandra.db.lifecycle.LifecycleNewTracker;
import org.apache.cassandra.index.Index;
import org.apache.cassandra.io.FSWriteError;
import org.apache.cassandra.io.sstable.AbstractRowIndexEntry;
import org.apache.cassandra.io.sstable.Descriptor;
import org.apache.cassandra.io.sstable.Downsampling;
import org.apache.cassandra.io.sstable.SSTable;
import org.apache.cassandra.io.sstable.format.DataComponent;
import org.apache.cassandra.io.sstable.format.IndexComponent;
import org.apache.cassandra.io.sstable.format.SSTableReader;
import org.apache.cassandra.io.sstable.format.SortedTableWriter;
import org.apache.cassandra.io.sstable.format.big.BigFormat.Components;
import org.apache.cassandra.io.sstable.indexsummary.IndexSummary;
import org.apache.cassandra.io.sstable.indexsummary.IndexSummaryBuilder;
import org.apache.cassandra.io.sstable.keycache.KeyCache;
import org.apache.cassandra.io.sstable.keycache.KeyCacheSupport;
import org.apache.cassandra.io.util.DataPosition;
import org.apache.cassandra.io.util.FileHandle;
import org.apache.cassandra.io.util.FileUtils;
import org.apache.cassandra.io.util.MmappedRegionsCache;
import org.apache.cassandra.io.util.SequentialWriter;
import org.apache.cassandra.service.CacheService;
import org.apache.cassandra.utils.ByteBufferUtil;
import org.apache.cassandra.utils.EstimatedHistogram;
import org.apache.cassandra.utils.IFilter;
import org.apache.cassandra.utils.JVMStabilityInspector;
import org.apache.cassandra.utils.Throwables;

import static com.google.common.base.Preconditions.checkNotNull;
import static com.google.common.base.Preconditions.checkState;
import static org.apache.cassandra.io.util.FileHandle.Builder.NO_LENGTH_OVERRIDE;
import static org.apache.cassandra.utils.Clock.Global.currentTimeMillis;

public class BigTableWriter extends SortedTableWriter&lt;BigFormatPartitionWriter, BigTableWriter.IndexWriter&gt;
{
<span class="fc" id="L71">    private static final Logger logger = LoggerFactory.getLogger(BigTableWriter.class);</span>

    private final RowIndexEntry.IndexSerializer rowIndexEntrySerializer;
<span class="fc" id="L74">    private final Map&lt;DecoratedKey, AbstractRowIndexEntry&gt; cachedKeys = new HashMap&lt;&gt;();</span>
    private final boolean shouldMigrateKeyCache;

    public BigTableWriter(Builder builder, LifecycleNewTracker lifecycleNewTracker, SSTable.Owner owner)
    {
<span class="fc" id="L79">        super(builder, lifecycleNewTracker, owner);</span>

<span class="fc" id="L81">        this.rowIndexEntrySerializer = builder.getRowIndexEntrySerializer();</span>
<span class="fc" id="L82">        checkNotNull(this.rowIndexEntrySerializer);</span>

<span class="pc bpc" id="L84" title="2 of 4 branches missed.">        this.shouldMigrateKeyCache = DatabaseDescriptor.shouldMigrateKeycacheOnCompaction()</span>
                                     &amp;&amp; lifecycleNewTracker instanceof ILifecycleTransaction
<span class="fc bfc" id="L86" title="All 2 branches covered.">                                     &amp;&amp; !((ILifecycleTransaction) lifecycleNewTracker).isOffline();</span>
<span class="fc" id="L87">    }</span>

    @Override
    protected void onStartPartition(DecoratedKey key)
    {
<span class="pc" id="L92">        notifyObservers(o -&gt; o.startPartition(key, partitionWriter.getInitialPosition(), indexWriter.writer.position()));</span>
<span class="fc" id="L93">    }</span>

    @Override
    protected RowIndexEntry createRowIndexEntry(DecoratedKey key, DeletionTime partitionLevelDeletion, long finishResult) throws IOException
    {
        // afterAppend() writes the partition key before the first RowIndexEntry - so we have to add it's
        // serialized size to the index-writer position
<span class="fc" id="L100">        long indexFilePosition = ByteBufferUtil.serializedSizeWithShortLength(key.getKey()) + indexWriter.writer.position();</span>

<span class="fc" id="L102">        RowIndexEntry entry = RowIndexEntry.create(partitionWriter.getInitialPosition(),</span>
                                                   indexFilePosition,
                                                   partitionLevelDeletion,
<span class="fc" id="L105">                                                   partitionWriter.getHeaderLength(),</span>
<span class="fc" id="L106">                                                   partitionWriter.getColumnIndexCount(),</span>
<span class="fc" id="L107">                                                   partitionWriter.indexInfoSerializedSize(),</span>
<span class="fc" id="L108">                                                   partitionWriter.indexSamples(),</span>
<span class="fc" id="L109">                                                   partitionWriter.offsets(),</span>
<span class="fc" id="L110">                                                   rowIndexEntrySerializer.indexInfoSerializer(),</span>
                                                   descriptor.version);

<span class="fc" id="L113">        indexWriter.append(key, entry, dataWriter.position(), partitionWriter.buffer());</span>

<span class="fc bfc" id="L115" title="All 2 branches covered.">        if (shouldMigrateKeyCache)</span>
        {
<span class="fc bfc" id="L117" title="All 2 branches covered.">            for (SSTableReader reader : ((ILifecycleTransaction) lifecycleNewTracker).originals())</span>
            {
<span class="pc bpc" id="L119" title="1 of 4 branches missed.">                if (reader instanceof KeyCacheSupport&lt;?&gt; &amp;&amp; ((KeyCacheSupport&lt;?&gt;) reader).getCachedPosition(key, false) != null)</span>
                {
<span class="fc" id="L121">                    cachedKeys.put(key, entry);</span>
<span class="fc" id="L122">                    break;</span>
                }
<span class="fc" id="L124">            }</span>
        }

<span class="fc" id="L127">        return entry;</span>
    }

    @SuppressWarnings({ &quot;resource&quot;, &quot;RedundantSuppression&quot; })
    private BigTableReader openInternal(IndexSummaryBuilder.ReadableBoundary boundary, SSTableReader.OpenReason openReason)
    {
<span class="pc bpc" id="L133" title="2 of 6 branches missed.">        assert boundary == null || (boundary.indexLength &gt; 0 &amp;&amp; boundary.dataLength &gt; 0);</span>

<span class="fc" id="L135">        IFilter filter = null;</span>
<span class="fc" id="L136">        IndexSummary indexSummary = null;</span>
<span class="fc" id="L137">        FileHandle dataFile = null;</span>
<span class="fc" id="L138">        FileHandle indexFile = null;</span>

<span class="fc" id="L140">        BigTableReader.Builder builder = unbuildTo(new BigTableReader.Builder(descriptor), true).setMaxDataAge(maxDataAge)</span>
<span class="fc" id="L141">                                                                                                .setSerializationHeader(header)</span>
<span class="fc" id="L142">                                                                                                .setOpenReason(openReason)</span>
<span class="fc" id="L143">                                                                                                .setFirst(first)</span>
<span class="fc bfc" id="L144" title="All 2 branches covered.">                                                                                                .setLast(boundary != null ? boundary.lastKey : last);</span>

        BigTableReader reader;
        try
        {

<span class="fc" id="L150">            builder.setStatsMetadata(statsMetadata());</span>

<span class="fc" id="L152">            EstimatedHistogram partitionSizeHistogram = builder.getStatsMetadata().estimatedPartitionSize;</span>
<span class="fc bfc" id="L153" title="All 2 branches covered.">            if (boundary != null)</span>
            {
<span class="pc bpc" id="L155" title="1 of 2 branches missed.">                if (partitionSizeHistogram.isOverflowed())</span>
                {
<span class="nc" id="L157">                    logger.warn(&quot;Estimated partition size histogram for '{}' is overflowed ({} values greater than {}). &quot; +</span>
                                &quot;Clearing the overflow bucket to allow for degraded mean and percentile calculations...&quot;,
<span class="nc" id="L159">                                descriptor, partitionSizeHistogram.overflowCount(), partitionSizeHistogram.getLargestBucketOffset());</span>
<span class="nc" id="L160">                    partitionSizeHistogram.clearOverflow();</span>
                }
            }

<span class="fc" id="L164">            filter = indexWriter.getFilterCopy();</span>
<span class="fc" id="L165">            builder.setFilter(filter);</span>
<span class="fc" id="L166">            indexSummary = indexWriter.summary.build(metadata().partitioner, boundary);</span>
<span class="fc" id="L167">            builder.setIndexSummary(indexSummary);</span>

<span class="fc" id="L169">            long indexFileLength = descriptor.fileFor(Components.PRIMARY_INDEX).length();</span>
<span class="fc" id="L170">            int indexBufferSize = ioOptions.diskOptimizationStrategy.bufferSize(indexFileLength / builder.getIndexSummary().size());</span>
<span class="fc" id="L171">            FileHandle.Builder indexFileBuilder = indexWriter.builder;</span>
<span class="fc" id="L172">            indexFile = indexFileBuilder.bufferSize(indexBufferSize)</span>
<span class="fc bfc" id="L173" title="All 2 branches covered.">                                        .withLengthOverride(boundary != null ? boundary.indexLength : NO_LENGTH_OVERRIDE)</span>
<span class="fc" id="L174">                                        .complete();</span>
<span class="fc" id="L175">            builder.setIndexFile(indexFile);</span>
<span class="fc bfc" id="L176" title="All 2 branches covered.">            dataFile = openDataFile(boundary != null ? boundary.dataLength : NO_LENGTH_OVERRIDE, builder.getStatsMetadata());</span>
<span class="fc" id="L177">            builder.setDataFile(dataFile);</span>
<span class="pc bpc" id="L178" title="1 of 2 branches missed.">            builder.setKeyCache(metadata().params.caching.cacheKeys() ? new KeyCache(CacheService.instance.keyCache) : KeyCache.NO_CACHE);</span>

<span class="fc" id="L180">            reader = builder.build(owner().orElse(null), true, true);</span>
        }
<span class="nc" id="L182">        catch (Throwable t)</span>
        {
<span class="nc" id="L184">            JVMStabilityInspector.inspectThrowable(t);</span>
<span class="nc" id="L185">            Throwables.closeNonNullAndAddSuppressed(t, dataFile, indexFile, indexSummary, filter);</span>
<span class="nc" id="L186">            throw t;</span>
<span class="fc" id="L187">        }</span>

        try
        {
<span class="fc bfc" id="L191" title="All 2 branches covered.">            for (Map.Entry&lt;DecoratedKey, AbstractRowIndexEntry&gt; cachedKey : cachedKeys.entrySet())</span>
<span class="fc" id="L192">                reader.cacheKey(cachedKey.getKey(), cachedKey.getValue());</span>

            // clearing the collected cache keys so that we will not have to cache them again when opening partial or
            // final later - cache key refer only to the descriptor, not to the particular SSTableReader instance.
<span class="fc" id="L196">            cachedKeys.clear();</span>
        }
<span class="nc" id="L198">        catch (Throwable t)</span>
        {
<span class="nc" id="L200">            JVMStabilityInspector.inspectThrowable(t);</span>
<span class="fc" id="L201">        }</span>

<span class="fc" id="L203">        return reader;</span>
    }

    @Override
    public void openEarly(Consumer&lt;SSTableReader&gt; doWhenReady)
    {
        // find the max (exclusive) readable key
<span class="fc" id="L210">        IndexSummaryBuilder.ReadableBoundary boundary = indexWriter.getMaxReadable();</span>
<span class="pc bpc" id="L211" title="1 of 2 branches missed.">        if (boundary == null)</span>
<span class="nc" id="L212">            return;</span>

<span class="fc" id="L214">        doWhenReady.accept(openInternal(boundary, SSTableReader.OpenReason.EARLY));</span>
<span class="fc" id="L215">    }</span>

    @Override
    public SSTableReader openFinalEarly()
    {
        // we must ensure the data is completely flushed to disk
<span class="fc" id="L221">        dataWriter.sync();</span>
<span class="fc" id="L222">        indexWriter.writer.sync();</span>

<span class="fc" id="L224">        return openFinal(SSTableReader.OpenReason.EARLY);</span>
    }

    @Override
    public SSTableReader openFinal(SSTableReader.OpenReason openReason)
    {
<span class="fc bfc" id="L230" title="All 2 branches covered.">        if (maxDataAge &lt; 0)</span>
<span class="fc" id="L231">            maxDataAge = currentTimeMillis();</span>

<span class="fc" id="L233">        return openInternal(null, openReason);</span>
    }

    /**
     * Encapsulates writing the index and filter for an SSTable. The state of this object is not valid until it has been closed.
     */
    protected static class IndexWriter extends SortedTableWriter.AbstractIndexWriter
    {
        private final RowIndexEntry.IndexSerializer rowIndexEntrySerializer;

        final SequentialWriter writer;
        final FileHandle.Builder builder;
        final IndexSummaryBuilder summary;
        private DataPosition mark;
        private DecoratedKey first;
        private DecoratedKey last;

        protected IndexWriter(Builder b, SequentialWriter dataWriter)
        {
<span class="fc" id="L252">            super(b);</span>
<span class="fc" id="L253">            this.rowIndexEntrySerializer = b.getRowIndexEntrySerializer();</span>
<span class="fc" id="L254">            writer = new SequentialWriter(b.descriptor.fileFor(Components.PRIMARY_INDEX), b.getIOOptions().writerOptions);</span>
<span class="fc" id="L255">            builder = IndexComponent.fileBuilder(Components.PRIMARY_INDEX, b).withMmappedRegionsCache(b.getMmappedRegionsCache());</span>
<span class="fc" id="L256">            summary = new IndexSummaryBuilder(b.getKeyCount(), b.getTableMetadataRef().getLocal().params.minIndexInterval, Downsampling.BASE_SAMPLING_LEVEL);</span>
            // register listeners to be alerted when the data files are flushed
<span class="fc" id="L258">            writer.setPostFlushListener(summary::markIndexSynced);</span>
<span class="fc" id="L259">            dataWriter.setPostFlushListener(summary::markDataSynced);</span>
<span class="fc" id="L260">        }</span>

        // finds the last (-offset) decorated key that can be guaranteed to occur fully in the flushed portion of the index file
        IndexSummaryBuilder.ReadableBoundary getMaxReadable()
        {
<span class="fc" id="L265">            return summary.getLastReadableBoundary();</span>
        }

        public void append(DecoratedKey key, RowIndexEntry indexEntry, long dataEnd, ByteBuffer indexInfo) throws IOException
        {
<span class="fc" id="L270">            bf.add(key);</span>
<span class="fc bfc" id="L271" title="All 2 branches covered.">            if (first == null)</span>
<span class="fc" id="L272">                first = key;</span>
<span class="fc" id="L273">            last = key;</span>

<span class="fc" id="L275">            long indexStart = writer.position();</span>
            try
            {
<span class="fc" id="L278">                ByteBufferUtil.writeWithShortLength(key.getKey(), writer);</span>
<span class="fc" id="L279">                rowIndexEntrySerializer.serialize(indexEntry, writer, indexInfo);</span>
            }
<span class="nc" id="L281">            catch (IOException e)</span>
            {
<span class="nc" id="L283">                throw new FSWriteError(e, writer.getPath());</span>
<span class="fc" id="L284">            }</span>
<span class="fc" id="L285">            long indexEnd = writer.position();</span>

<span class="pc bpc" id="L287" title="1 of 2 branches missed.">            if (logger.isTraceEnabled())</span>
<span class="nc" id="L288">                logger.trace(&quot;wrote index entry: {} at {}&quot;, indexEntry, indexStart);</span>

<span class="fc" id="L290">            summary.maybeAddEntry(key, indexStart, indexEnd, dataEnd);</span>
<span class="fc" id="L291">        }</span>

        @Override
        public void mark()
        {
<span class="nc" id="L296">            mark = writer.mark();</span>
<span class="nc" id="L297">        }</span>

        @Override
        public void resetAndTruncate()
        {
            // we can't un-set the bloom filter addition, but extra keys in there are harmless.
            // we can't reset dbuilder either, but that is the last thing called in afterappend, so
            // we assume that if that worked then we won't be trying to reset.
<span class="nc" id="L305">            writer.resetAndTruncate(mark);</span>
<span class="nc" id="L306">        }</span>

        protected void doPrepare()
        {
<span class="fc" id="L310">            checkNotNull(first);</span>
<span class="fc" id="L311">            checkNotNull(last);</span>

<span class="fc" id="L313">            super.doPrepare();</span>

            // truncate index file
<span class="fc" id="L316">            long position = writer.position();</span>
<span class="fc" id="L317">            writer.prepareToCommit();</span>
<span class="fc" id="L318">            FileUtils.truncate(writer.getPath(), position);</span>

            // save summary
<span class="fc" id="L321">            summary.prepareToCommit();</span>
<span class="fc" id="L322">            try (IndexSummary indexSummary = summary.build(metadata.getLocal().partitioner))</span>
            {
<span class="fc" id="L324">                new IndexSummaryComponent(indexSummary, first, last).save(descriptor.fileFor(Components.SUMMARY), true);</span>
            }
<span class="nc" id="L326">            catch (IOException ex)</span>
            {
<span class="nc" id="L328">                logger.warn(&quot;Failed to save index summary&quot;, ex);</span>
<span class="fc" id="L329">            }</span>
<span class="fc" id="L330">        }</span>

        protected Throwable doCommit(Throwable accumulate)
        {
<span class="fc" id="L334">            return writer.commit(accumulate);</span>
        }

        protected Throwable doAbort(Throwable accumulate)
        {
<span class="nc" id="L339">            return summary.close(writer.abort(accumulate));</span>
        }

        @Override
        protected Throwable doPostCleanup(Throwable accumulate)
        {
<span class="fc" id="L345">            accumulate = super.doPostCleanup(accumulate);</span>
<span class="fc" id="L346">            accumulate = summary.close(accumulate);</span>
<span class="fc" id="L347">            return accumulate;</span>
        }
    }

    public static class Builder extends SortedTableWriter.Builder&lt;BigFormatPartitionWriter, IndexWriter, BigTableWriter, Builder&gt;
    {
        private RowIndexEntry.IndexSerializer rowIndexEntrySerializer;
        private MmappedRegionsCache mmappedRegionsCache;
        private OperationType operationType;

        // Writers are expected to be opened only once during construction of the sstable. The following flags are used
        // to ensure that.
        private boolean indexWriterOpened;
        private boolean dataWriterOpened;
        private boolean partitionWriterOpened;

        public Builder(Descriptor descriptor)
        {
<span class="fc" id="L365">            super(descriptor);</span>
<span class="fc" id="L366">        }</span>

        @Override
        public Builder addDefaultComponents(Collection&lt;Index.Group&gt; indexGroups)
        {
<span class="fc" id="L371">            super.addDefaultComponents(indexGroups);</span>

<span class="fc" id="L373">            addComponents(ImmutableSet.of(Components.PRIMARY_INDEX, Components.SUMMARY));</span>

<span class="fc" id="L375">            return this;</span>
        }

        // The following getters for the resources opened by buildInternal method can be only used during the lifetime of
        // that method - that is, during the construction of the sstable.

        @Override
        public MmappedRegionsCache getMmappedRegionsCache()
        {
<span class="fc" id="L384">            return ensuringInBuildInternalContext(mmappedRegionsCache);</span>
        }

        @Override
        protected SequentialWriter openDataWriter()
        {
<span class="pc bpc" id="L390" title="1 of 2 branches missed.">            checkState(!dataWriterOpened, &quot;Data writer has been already opened.&quot;);</span>

<span class="fc" id="L392">            SequentialWriter dataWriter = DataComponent.buildWriter(descriptor,</span>
<span class="fc" id="L393">                                                                    getTableMetadataRef().getLocal(),</span>
<span class="fc" id="L394">                                                                    getIOOptions().writerOptions,</span>
<span class="fc" id="L395">                                                                    getMetadataCollector(),</span>
<span class="fc" id="L396">                                                                    ensuringInBuildInternalContext(operationType),</span>
<span class="fc" id="L397">                                                                    getIOOptions().flushCompression);</span>
<span class="fc" id="L398">            this.dataWriterOpened = true;</span>
<span class="fc" id="L399">            return dataWriter;</span>
        }

        @Override
        protected IndexWriter openIndexWriter(SequentialWriter dataWriter)
        {
<span class="fc" id="L405">            checkNotNull(dataWriter);</span>
<span class="pc bpc" id="L406" title="1 of 2 branches missed.">            checkState(!indexWriterOpened, &quot;Index writer has been already opened.&quot;);</span>

<span class="fc" id="L408">            IndexWriter indexWriter = new IndexWriter(this, dataWriter);</span>
<span class="fc" id="L409">            this.indexWriterOpened = true;</span>
<span class="fc" id="L410">            return indexWriter;</span>
        }

        @Override
        protected BigFormatPartitionWriter openPartitionWriter(SequentialWriter dataWriter, IndexWriter indexWriter)
        {
<span class="fc" id="L416">            checkNotNull(dataWriter);</span>
<span class="fc" id="L417">            checkNotNull(indexWriter);</span>
<span class="pc bpc" id="L418" title="1 of 2 branches missed.">            checkState(!partitionWriterOpened, &quot;Partition writer has been already opened.&quot;);</span>

<span class="fc" id="L420">            BigFormatPartitionWriter partitionWriter = new BigFormatPartitionWriter(getSerializationHeader(), dataWriter, descriptor.version, getRowIndexEntrySerializer().indexInfoSerializer());</span>
<span class="fc" id="L421">            this.partitionWriterOpened = true;</span>
<span class="fc" id="L422">            return partitionWriter;</span>
        }

        RowIndexEntry.IndexSerializer getRowIndexEntrySerializer()
        {
<span class="fc" id="L427">            return ensuringInBuildInternalContext(rowIndexEntrySerializer);</span>
        }

        private &lt;T&gt; T ensuringInBuildInternalContext(T value)
        {
<span class="pc bpc" id="L432" title="1 of 2 branches missed.">            checkState(value != null, &quot;The requested resource has not been initialized yet.&quot;);</span>
<span class="fc" id="L433">            return value;</span>
        }

        @Override
        protected BigTableWriter buildInternal(LifecycleNewTracker lifecycleNewTracker, Owner owner)
        {
            try
            {
<span class="fc" id="L441">                this.operationType = lifecycleNewTracker.opType();</span>
<span class="fc" id="L442">                this.mmappedRegionsCache = new MmappedRegionsCache();</span>
<span class="fc bfc" id="L443" title="All 2 branches covered.">                this.rowIndexEntrySerializer = new RowIndexEntry.Serializer(descriptor.version, getSerializationHeader(), owner != null ? owner.getMetrics() : null);</span>
<span class="fc" id="L444">                return new BigTableWriter(this, lifecycleNewTracker, owner);</span>
            }
<span class="nc" id="L446">            catch (RuntimeException | Error ex)</span>
            {
<span class="nc" id="L448">                Throwables.closeNonNullAndAddSuppressed(ex, mmappedRegionsCache);</span>
<span class="nc" id="L449">                throw ex;</span>
            }
            finally
            {
<span class="fc" id="L453">                this.rowIndexEntrySerializer = null;</span>
<span class="fc" id="L454">                this.mmappedRegionsCache = null;</span>
<span class="fc" id="L455">                this.operationType = null;</span>
<span class="fc" id="L456">                this.partitionWriterOpened = false;</span>
<span class="fc" id="L457">                this.indexWriterOpened = false;</span>
<span class="fc" id="L458">                this.dataWriterOpened = false;</span>
            }
        }
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>
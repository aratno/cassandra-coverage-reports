<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ReplicaLayout.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">JaCoCo Cassandara Coverage Report</a> &gt; <a href="index.source.html" class="el_package">org.apache.cassandra.locator</a> &gt; <span class="el_source">ReplicaLayout.java</span></div><h1>ReplicaLayout.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.locator;

import com.google.common.annotations.VisibleForTesting;
import org.apache.cassandra.config.DatabaseDescriptor;
import org.apache.cassandra.db.Keyspace;
import org.apache.cassandra.db.PartitionPosition;
import org.apache.cassandra.dht.AbstractBounds;
import org.apache.cassandra.dht.Token;
import org.apache.cassandra.gms.FailureDetector;
import org.apache.cassandra.utils.FBUtilities;

import java.util.Set;
import java.util.function.Predicate;

/**
 * The relevant replicas for an operation over a given range or token.
 *
 * @param &lt;E&gt;
 */
<span class="fc" id="L38">public abstract class ReplicaLayout&lt;E extends Endpoints&lt;E&gt;&gt;</span>
{
    private final E natural;
    // the snapshot of the replication strategy that corresponds to the replica layout
    private final AbstractReplicationStrategy replicationStrategy;

    ReplicaLayout(AbstractReplicationStrategy replicationStrategy, E natural)
<span class="fc" id="L45">    {</span>
<span class="fc" id="L46">        this.replicationStrategy = replicationStrategy;</span>
<span class="fc" id="L47">        this.natural = natural;</span>
<span class="fc" id="L48">    }</span>

    /**
     * The 'natural' owners of the ring position(s), as implied by the current ring layout.
     * This excludes any pending owners, i.e. those that are in the process of taking ownership of a range, but
     * have not yet finished obtaining their view of the range.
     */
    public final E natural()
    {
<span class="fc" id="L57">        return natural;</span>
    }

    public final AbstractReplicationStrategy replicationStrategy()
    {
<span class="fc" id="L62">        return replicationStrategy;</span>
    }

    /**
     * All relevant owners of the ring position(s) for this operation, as implied by the current ring layout.
     * For writes, this will include pending owners, and for reads it will be equivalent to natural()
     */
    public E all()
    {
<span class="nc" id="L71">        return natural;</span>
    }

    public String toString()
    {
<span class="nc" id="L76">        return &quot;ReplicaLayout [ natural: &quot; + natural + &quot; ]&quot;;</span>
    }

    public static class ForTokenRead extends ReplicaLayout&lt;EndpointsForToken&gt; implements ForToken
    {
        public ForTokenRead(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural)
        {
<span class="fc" id="L83">            super(replicationStrategy, natural);</span>
<span class="fc" id="L84">        }</span>

        @Override
        public Token token()
        {
<span class="nc" id="L89">            return natural().token();</span>
        }

        public ReplicaLayout.ForTokenRead filter(Predicate&lt;Replica&gt; filter)
        {
<span class="nc" id="L94">            EndpointsForToken filtered = natural().filter(filter);</span>
            // AbstractReplicaCollection.filter returns itself if all elements match the filter
<span class="nc bnc" id="L96" title="All 2 branches missed.">            if (filtered == natural()) return this;</span>
<span class="nc" id="L97">            return new ReplicaLayout.ForTokenRead(replicationStrategy(), filtered);</span>
        }
    }

    public static class ForRangeRead extends ReplicaLayout&lt;EndpointsForRange&gt; implements ForRange
    {
        final AbstractBounds&lt;PartitionPosition&gt; range;

        public ForRangeRead(AbstractReplicationStrategy replicationStrategy, AbstractBounds&lt;PartitionPosition&gt; range, EndpointsForRange natural)
        {
<span class="fc" id="L107">            super(replicationStrategy, natural);</span>
<span class="fc" id="L108">            this.range = range;</span>
<span class="fc" id="L109">        }</span>

        @Override
        public AbstractBounds&lt;PartitionPosition&gt; range()
        {
<span class="nc" id="L114">            return range;</span>
        }

        public ReplicaLayout.ForRangeRead filter(Predicate&lt;Replica&gt; filter)
        {
<span class="nc" id="L119">            EndpointsForRange filtered = natural().filter(filter);</span>
            // AbstractReplicaCollection.filter returns itself if all elements match the filter
<span class="nc bnc" id="L121" title="All 2 branches missed.">            if (filtered == natural()) return this;</span>
<span class="nc" id="L122">            return new ReplicaLayout.ForRangeRead(replicationStrategy(), range(), filtered);</span>
        }
    }

<span class="fc" id="L126">    public static class ForWrite&lt;E extends Endpoints&lt;E&gt;&gt; extends ReplicaLayout&lt;E&gt;</span>
    {
        final E all;
        final E pending;

        ForWrite(AbstractReplicationStrategy replicationStrategy, E natural, E pending, E all)
        {
<span class="fc" id="L133">            super(replicationStrategy, natural);</span>
<span class="pc bpc" id="L134" title="2 of 4 branches missed.">            assert pending != null &amp;&amp; !haveWriteConflicts(natural, pending);</span>
<span class="fc bfc" id="L135" title="All 2 branches covered.">            if (all == null)</span>
<span class="fc" id="L136">                all = Endpoints.concat(natural, pending);</span>
<span class="fc" id="L137">            this.all = all;</span>
<span class="fc" id="L138">            this.pending = pending;</span>
<span class="fc" id="L139">        }</span>

        public final E all()
        {
<span class="fc" id="L143">            return all;</span>
        }

        public final E pending()
        {
<span class="fc" id="L148">            return pending;</span>
        }

        public String toString()
        {
<span class="nc" id="L153">            return &quot;ReplicaLayout [ natural: &quot; + natural() + &quot;, pending: &quot; + pending + &quot; ]&quot;;</span>
        }
    }

    public static class ForTokenWrite extends ForWrite&lt;EndpointsForToken&gt; implements ForToken
    {
        public ForTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural, EndpointsForToken pending)
        {
<span class="fc" id="L161">            this(replicationStrategy, natural, pending, null);</span>
<span class="fc" id="L162">        }</span>
        public ForTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural, EndpointsForToken pending, EndpointsForToken all)
        {
<span class="fc" id="L165">            super(replicationStrategy, natural, pending, all);</span>
<span class="fc" id="L166">        }</span>

        @Override
<span class="nc" id="L169">        public Token token() { return natural().token(); }</span>

        public ForTokenWrite filter(Predicate&lt;Replica&gt; filter)
        {
<span class="fc" id="L173">            EndpointsForToken filtered = all().filter(filter);</span>
            // AbstractReplicaCollection.filter returns itself if all elements match the filter
<span class="fc bfc" id="L175" title="All 2 branches covered.">            if (filtered == all()) return this;</span>
<span class="pc bpc" id="L176" title="1 of 2 branches missed.">            if (pending().isEmpty()) return new ForTokenWrite(replicationStrategy(), filtered, pending(), filtered);</span>
            // unique by endpoint, so can for efficiency filter only on endpoint
<span class="nc" id="L178">            return new ForTokenWrite(</span>
<span class="nc" id="L179">                    replicationStrategy(),</span>
<span class="nc" id="L180">                    natural().keep(filtered.endpoints()),</span>
<span class="nc" id="L181">                    pending().keep(filtered.endpoints()),</span>
                    filtered
            );
        }
    }

    public interface ForRange
    {
        public AbstractBounds&lt;PartitionPosition&gt; range();
    }

    public interface ForToken
    {
        public Token token();
    }

    /**
     * Gets the 'natural' and 'pending' replicas that own a given token, with no filtering or processing.
     *
     * Since a write is intended for all nodes (except, unless necessary, transient replicas), this method's
     * only responsibility is to fetch the 'natural' and 'pending' replicas, then resolve any conflicts
     * {@link ReplicaLayout#haveWriteConflicts(Endpoints, Endpoints)}
     */
    public static ReplicaLayout.ForTokenWrite forTokenWriteLiveAndDown(Keyspace keyspace, Token token)
    {
        // TODO: these should be cached, not the natural replicas
        // TODO: race condition to fetch these. implications??
<span class="fc" id="L208">        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();</span>
<span class="fc" id="L209">        EndpointsForToken natural = EndpointsForToken.natural(replicationStrategy, token);</span>
<span class="fc" id="L210">        EndpointsForToken pending = EndpointsForToken.pending(keyspace, token);</span>
<span class="fc" id="L211">        return forTokenWrite(replicationStrategy, natural, pending);</span>
    }

    public static ReplicaLayout.ForTokenWrite forTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural, EndpointsForToken pending)
    {
<span class="pc bpc" id="L216" title="1 of 2 branches missed.">        if (haveWriteConflicts(natural, pending))</span>
        {
<span class="nc" id="L218">            natural = resolveWriteConflictsInNatural(natural, pending);</span>
<span class="nc" id="L219">            pending = resolveWriteConflictsInPending(natural, pending);</span>
        }
<span class="fc" id="L221">        return new ReplicaLayout.ForTokenWrite(replicationStrategy, natural, pending);</span>
    }

    /**
     * Detect if we have any endpoint in both pending and full; this can occur either due to races (there is no isolation)
     * or because an endpoint is transitioning between full and transient replication status.
     *
     * We essentially always prefer the full version for writes, because this is stricter.
     *
     * For transient-&gt;full transitions:
     *
     *   Since we always write to any pending transient replica, effectively upgrading it to full for the transition duration,
     *   it might at first seem to be OK to continue treating the conflict replica as its 'natural' transient form,
     *   as there is always a quorum of nodes receiving the write.  However, ring ownership changes are not atomic or
     *   consistent across the cluster, and it is possible for writers to see different ring states.
     *
     *   Furthermore, an operator would expect that the full node has received all writes, with no extra need for repair
     *   (as the normal contract dictates) when it completes its transition.
     *
     *   While we cannot completely eliminate risks due to ring inconsistencies, this approach is the most conservative
     *   available to us today to mitigate, and (we think) the easiest to reason about.
     *
     * For full-&gt;transient transitions:
     *
     *   In this case, things are dicier, because in theory we can trigger this change instantly.  All we need to do is
     *   drop some data, surely?
     *
     *   Ring movements can put us in a pickle; any other node could believe us to be full when we have become transient,
     *   and perform a full data request to us that we believe ourselves capable of answering, but that we are not.
     *   If the ring is inconsistent, it's even feasible that a transient request would be made to the node that is losing
     *   its transient status, that also does not know it has yet done so, resulting in all involved nodes being unaware
     *   of the data inconsistency.
     *
     *   This happens because ring ownership changes are implied by a single node; not all owning nodes get a say in when
     *   the transition takes effect.  As such, a node can hold an incorrect belief about its own ownership ranges.
     *
     *   This race condition is somewhat inherent in present day Cassandra, and there's actually a limit to what we can do about it.
     *   It is a little more dangerous with transient replication, however, because we can completely answer a request without
     *   ever touching a digest, meaning we are less likely to attempt to repair any inconsistency.
     *
     *   We aren't guaranteed to contact any different nodes for the data requests, of course, though we at least have a chance.
     *
     * Note: If we have any pending transient-&gt;full movement, we need to move the full replica to our 'natural' bucket
     * to avoid corrupting our count.  This is fine for writes, all we're doing is ensuring we always write to the node,
     * instead of selectively.
     *
     * @param natural
     * @param pending
     * @param &lt;E&gt;
     * @return
     */
    static &lt;E extends Endpoints&lt;E&gt;&gt; boolean haveWriteConflicts(E natural, E pending)
    {
<span class="fc" id="L274">        Set&lt;InetAddressAndPort&gt; naturalEndpoints = natural.endpoints();</span>
<span class="fc bfc" id="L275" title="All 2 branches covered.">        for (InetAddressAndPort pendingEndpoint : pending.endpoints())</span>
        {
<span class="pc bpc" id="L277" title="1 of 2 branches missed.">            if (naturalEndpoints.contains(pendingEndpoint))</span>
<span class="nc" id="L278">                return true;</span>
<span class="fc" id="L279">        }</span>
<span class="fc" id="L280">        return false;</span>
    }

    /**
     * MUST APPLY FIRST
     * See {@link ReplicaLayout#haveWriteConflicts}
     * @return a 'natural' replica collection, that has had its conflicts with pending repaired
     */
    @VisibleForTesting
    static EndpointsForToken resolveWriteConflictsInNatural(EndpointsForToken natural, EndpointsForToken pending)
    {
<span class="nc" id="L291">        EndpointsForToken.Builder resolved = natural.newBuilder(natural.size());</span>
<span class="nc bnc" id="L292" title="All 2 branches missed.">        for (Replica replica : natural)</span>
        {
            // always prefer the full natural replica, if there is a conflict
<span class="nc bnc" id="L295" title="All 2 branches missed.">            if (replica.isTransient())</span>
            {
<span class="nc" id="L297">                Replica conflict = pending.byEndpoint().get(replica.endpoint());</span>
<span class="nc bnc" id="L298" title="All 2 branches missed.">                if (conflict != null)</span>
                {
                    // it should not be possible to have conflicts of the same replication type for the same range
<span class="nc bnc" id="L301" title="All 2 branches missed.">                    assert conflict.isFull();</span>
                    // If we have any pending transient-&gt;full movement, we need to move the full replica to our 'natural' bucket
                    // to avoid corrupting our count
<span class="nc" id="L304">                    resolved.add(conflict);</span>
<span class="nc" id="L305">                    continue;</span>
                }
            }
<span class="nc" id="L308">            resolved.add(replica);</span>
<span class="nc" id="L309">        }</span>
<span class="nc" id="L310">        return resolved.build();</span>
    }

    /**
     * MUST APPLY SECOND
     * See {@link ReplicaLayout#haveWriteConflicts}
     * @return a 'pending' replica collection, that has had its conflicts with natural repaired
     */
    @VisibleForTesting
    static EndpointsForToken resolveWriteConflictsInPending(EndpointsForToken natural, EndpointsForToken pending)
    {
<span class="nc" id="L321">        return pending.without(natural.endpoints());</span>
    }

    /**
     * @return the read layout for a token - this includes only live natural replicas, i.e. those that are not pending
     * and not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch
     */
    public static ReplicaLayout.ForTokenRead forTokenReadLiveSorted(AbstractReplicationStrategy replicationStrategy, Token token)
    {
<span class="fc" id="L330">        EndpointsForToken replicas = replicationStrategy.getNaturalReplicasForToken(token);</span>
<span class="fc" id="L331">        replicas = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), replicas);</span>
<span class="fc" id="L332">        replicas = replicas.filter(FailureDetector.isReplicaAlive);</span>
<span class="fc" id="L333">        return new ReplicaLayout.ForTokenRead(replicationStrategy, replicas);</span>
    }

    /**
     * TODO: we should really double check that the provided range does not overlap multiple token ring regions
     * @return the read layout for a range - this includes only live natural replicas, i.e. those that are not pending
     * and not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch
     */
    static ReplicaLayout.ForRangeRead forRangeReadLiveSorted(AbstractReplicationStrategy replicationStrategy, AbstractBounds&lt;PartitionPosition&gt; range)
    {
<span class="fc" id="L343">        EndpointsForRange replicas = replicationStrategy.getNaturalReplicas(range.right);</span>
<span class="fc" id="L344">        replicas = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), replicas);</span>
<span class="fc" id="L345">        replicas = replicas.filter(FailureDetector.isReplicaAlive);</span>
<span class="fc" id="L346">        return new ReplicaLayout.ForRangeRead(replicationStrategy, range, replicas);</span>
    }

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>